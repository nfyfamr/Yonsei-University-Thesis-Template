@inproceedings{Wong2014Support,
abstract = {Pointing gestures -- particularly deictic references -- are ubiquitous in face-to-face communication. However, deictic pointing can be much more difficult in collaborative virtual environments (CVEs) than in everyday life -- early studies found that the 'fragmentation' caused by the environment greatly complicated object-based communication. In the fifteen years since these studies appeared, the technologies used in CVEs have improved substantially, and several techniques for improving pointing have been proposed or implemented. What these advances mean for the problems of fragmentation and deictic gesture, however, is not clear. To find out, we conducted a new observational study of deictic pointing in a CVE with several techniques that may reduce fragmentation: extra-wide and third-person views, precise control over an avatar's pointing arm, and visual enhancements such as object highlighting and laser pointing. Our study shows that although pointing has come a long way, problems of fragmentation still occur, and that visual and view enhancements can cause new problems for collaboration, even as they solve others. In addition, the visibility of a gesture's preparatory actions remained important to study participants, even when pointing was augmented. These results provide a richer understanding of the subtlety in avatar-based deictic communication, and of the ways that this critical communication mechanism can be better supported in CVEs.},
address = {New York, New York, USA},
author = {Wong, Nelson and Gutwin, Carl},
booktitle = {Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work \& Social Computing},
doi = {10.1145/2531602.2531691},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wong, Gutwin - 2014 - Support for deictic pointing in CVEs.pdf:pdf},
isbn = {9781450325400},
keywords = {avatars,cves,gestures,pointing},
pages = {1377--1387},
publisher = {Association for Computing Machinery},
series = {CSCW '14},
title = {{Support for Deictic Pointing in CVEs: Still Fragmented after All These Years'}},
year = {2014}
}
@inproceedings{Tang2010Three,
abstract = {We explore the design of a system for three-way collaboration over a shared visual workspace, specifically in how to support three channels of communication: person, reference, and task-space. In two studies, we explore the implications of extending designs intended for dyadic collaboration to three-person groups, and the role of each communication channel. Our studies illustrate the utility of multiple configurations of users around a distributed workspace, and explore the subtleties of traditional notions of identity, awareness, spatial metaphor, and corporeal embodiments as they relate to three-way collaboration. Copyright 2010 ACM.},
address = {New York, New York, USA},
author = {Tang, Anthony and Pahud, Michel and Inkpen, Kori and Benko, Hrvoje and Tang, John C. and Buxton, Bill},
booktitle = {Proceedings of the 2010 ACM conference on Computer supported cooperative work - CSCW '10},
doi = {10.1145/1718918.1718969},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Piumsomboon, Lee, Billinghurst - 2018 - Snow dome A multi-scale interaction in mixed reality remote collaboration.pdf:pdf},
isbn = {9781605587950},
keywords = {Media space,Shared workspace,Tabletop,Video-mediated communication},
pages = {271--280},
publisher = {Association for Computing Machinery},
series = {CSCW '10},
title = {{Three's Company: Understanding Communication Channels in Three-Way Distributed Collaboration}},
year = {2010}
}
@inproceedings{Gao2016Oriented,
abstract = {We present a Mixed Reality system for remote collaboration using Virtual Reality (VR) headsets with external depth cameras attached. By wirelessly sharing a 3D point-cloud data of a local workers' workspace with a remote helper, and sharing the remote helper's hand gestures back to the local worker, the remote helper is able to assist the worker to perform manual tasks. Displaying the pointcloud video in a conventional way, such as a static front view in VR headsets, does not provide helpers with sufficient understanding of the spatial relationships between their hands and the remote surroundings. In contrast, we propose a Mixed Reality (MR) system that shares with the remote helper, not only 3D captured environment data but also real-time orientation info of the worker's viewpoint. We conducted a pilot study to evaluate the usability of the system, and we found that extra synchronized orientation data can make collaborators feel more connected spatially and mentally.},
address = {New York, New York, USA},
author = {Gao, Lei and Bai, Huidong and Lee, Gun and Billinghurst, Mark},
booktitle = {SIGGRAPH ASIA 2016 Mobile Graphics and Interactive Applications},
doi = {10.1145/2999508.2999531},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gao et al. - 2016 - An oriented point-cloud view for MR remote collaboration.pdf:pdf},
isbn = {9781450345514},
keywords = {Mixed Reality,Point cloud,RGBD camera,Remote collaboration,Tele-presence},
pages = {1--4},
publisher = {Association for Computing Machinery},
series = {SA '16},
title = {{An Oriented Point-Cloud View for MR Remote Collaboration}},
year = {2016}
}
@inproceedings{Gurevich2012TeleAdvisor,
abstract = {TeleAdvisor is a novel solution designed to support remote assistance tasks in many real-world scenarios. It consists of a video camera and a small projector mounted at the end of a tele-operated robotic arm. This enables a remote helper to view and interact with the workers' workspace, while controlling the point of view. It also provides the worker with a hands-free transportable device to be placed anywhere in his or her environment. Active tracking of the projection space is used in order to reliably correlate between the camera's view and the projector space. Copyright 2012 ACM.},
address = {New York, New York, USA},
author = {Gurevich, Pavel and Lanir, Joel and Cohen, Benjamin and Stone, Ran},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
doi = {10.1145/2207676.2207763},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gurevich et al. - 2012 - TeleAdvisor A versatile augmented reality tool for remote assistance.pdf:pdf},
isbn = {9781450310154},
keywords = {Augmented reality,Mobile projector,Remote assistance},
pages = {619--622},
publisher = {Association for Computing Machinery},
series = {CHI '12},
title = {{TeleAdvisor: A Versatile Augmented Reality Tool for Remote Assistance}},
year = {2012}
}
@inproceedings{Zillner20143D,
abstract = {This paper presents 3D-Board, a digital whiteboard capable of capturing life-sized virtual embodiments of geographically distributed users. When using large-scale screens for remote collaboration, awareness for the distributed users' gestures and actions is of particular importance. Our work adds to the literature on remote collaborative workspaces, it facilitates intuitive remote collaboration on large scale interactive whiteboards by preserving awareness of the full-body pose and gestures of the remote collaborator. By blending the front-facing 3D embodiment of a remote collaborator with the shared workspace, an illusion is created as if the observer was looking through the transparent whiteboard into the remote user's room. The system was tested and verified in a usability assessment, showing that 3D-Board significantly improves the effectiveness of remote collaboration on a large interactive surface.},
address = {New York, New York, USA},
author = {Zillner, Jakob and Rhemann, Christoph and Izadi, Shahram and Haller, Michael},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/2642918.2647393},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zillner et al. - 2014 - 3D-Board A wholebody remote collaborative whiteboard.pdf:pdf},
isbn = {9781450330695},
keywords = {Interactive whiteboard,Reconstruction,Remote collaboration,Shared workspace,Teleconferencing},
month = {oct},
pages = {471--480},
publisher = {Association for Computing Machinery},
series = {UIST '14},
title = {{3D-Board: A Whole-Body Remote Collaborative Whiteboard}},
year = {2014}
}
@techreport{Younghwan2020Remote,
author = {권영환 and 박태형 and 서영희 and 송지환 and 이중엽 and 진회승},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/영환 et al. - 2020 - 원격근무 솔루션 기술 {\textperiodcentered} 시장 동향 및 시사점.pdf:pdf},
institution = {SPRi},
pages = {1--35},
title = {원격근무 솔루션 기술 {\textperiodcentered} 시장 동향 및 시사점},
year = {2020}
}
@inproceedings{Fakourfar2016Stabilized,
abstract = {Recent mobile technology has provided new opportunities for creating remote assistance systems. However, mobile support systems present a particular challenge: both the camera and display are held by the user, leading to shaky video. When pointing or drawing annotations, this means that the desired target often moves, causing the gesture to lose its intended meaning. To address this problem, we investigate annotation stabilization techniques, which allow annotations to stick to their intended location. We studied two annotation systems, using three different forms of annotations, with both tablets and head-mounted displays. Our analysis suggests that stabilized annotations and head-mounted displays are only beneficial in certain situations. However, the simplest approach of automatically freezing video while drawing annotations was surprisingly effective in facilitating the completion of remote assistance tasks.},
address = {New York, NY, USA},
author = {Fakourfar, Omid and Ta, Kevin and Tang, Richard and Bateman, Scott and Tang, Anthony},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/2858036.2858171},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fakourfar et al. - 2016 - Stabilized Annotations for Mobile Remote Assistance.pdf:pdf},
isbn = {9781450333627},
keywords = {annotation systems,augmented reality,head-mounted displays,mobile video conferencing,remote assistance},
month = {may},
pages = {1548--1560},
publisher = {Association for Computing Machinery},
series = {CHI '16},
title = {{Stabilized Annotations for Mobile Remote Assistance}},

year = {2016}
}
@inproceedings{Bauer1999Where,
abstract = {This paper reports on an empirical study aimed at evaluating the utility of a reality-augmenting telepointer in a wearable videoconference system. Results show that using this telepointer a remote expert can effectively guide and direct a field worker's manual activities. By analyzing verbal communication behavior and pointing gestures, we were able to determine that experts overwhelmingly preferred pointing for guiding workers through physical tasks.},
author = {Bauer, Martin and Kortuem, Gerd and Segall, Zary},
booktitle = {Digest of Papers. Third International Symposium on Wearable Computers},
doi = {10.1109/ISWC.1999.806696},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bauer, Kortuem, Segall - 1999 - 'Where are you pointing at' A study of remote collaboration in a wearable videoconference system.pdf:pdf},
isbn = {0-7695-0428-0},
pages = {151--158},
publisher = {IEEE},
title = {{"Where are you pointing at?" A study of remote collaboration in a wearable videoconference system}},

year = {1999}
}
@article{Gupta2016Do,
abstract = {We present results from research exploring the effect of sharing virtual gaze and pointing cues in a wearable interface for remote collaboration. A local worker wears a Head-mounted Camera, Eye-tracking camera and a Head-Mounted Display and shares video and virtual gaze information with a remote helper. The remote helper can provide feedback using a virtual pointer on the live video view. The prototype system was evaluated with a formal user study. Comparing four conditions, (1) NONE (no cue), (2) POINTER, (3) EYE-TRACKER and (4) BOTH (both pointer and eye-tracker cues), we observed that the task completion performance was best in the BOTH condition with a significant difference of POINTER and EYETRACKER individually. The use of eye-tracking and a pointer also significantly improved the co-presence felt between the users. We discuss the implications of this research and the limitations of the developed system that could be improved in further work.},
author = {Gupta, Kunal and Lee, Gun A. and Billinghurst, Mark},
doi = {10.1109/TVCG.2016.2593778},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gupta, Lee, Billinghurst - 2016 - Do you see what i see the effect of gaze tracking on task space remote collaboration.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Computer conferencing,Computer-supported collaborative work,teleconferencing,videoconferencing},
month = {nov},
number = {11},
pages = {2413--2422},
publisher = {IEEE},
title = {{Do You See What I See? The Effect of Gaze Tracking on Task Space Remote Collaboration}},

volume = {22},
year = {2016}
}
@inproceedings{Sodhi2013BeThere,
abstract = {We present BeThere, a proof-of-concept system designed to explore 3D input for mobile collaborative interactions. With BeThere, we explore 3D gestures and spatial input which allow remote users to perform a variety of virtual interactions in a local user's physical environment. Our system is completely self-contained and uses depth sensors to track the location of a user's fingers as well as to capture the 3D shape of objects in front of the sensor. We illustrate the unique capabilities of our system through a series of interactions that allow users to control and manipulate 3D virtual content. We also provide qualitative feedback from a preliminary user study which confirmed that users can complete a shared collaborative task using our system.},
address = {New York, NY, USA},
annote = {RGBD 카메라를 이용한 실감형 AR 협업},
author = {Sodhi, Rajinder S and Jones, Brett R and Forsyth, David and Bailey, Brian P and Maciocci, Giuliano},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
doi = {10.1145/2470654.2470679},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sodhi et al. - 2013 - BeThere 3D mobile collaboration with spatial input.pdf:pdf},
isbn = {9781450318990},
keywords = {around device interaction,augmented reality,collaboration,depth sensors},
month = {apr},
pages = {179--188},
publisher = {Association for Computing Machinery},
series = {CHI '13},
title = {{BeThere: 3D Mobile Collaboration with Spatial Input}},

year = {2013}
}
@inproceedings{Jones2014RoomAlive,
abstract = {RoomAlive is a proof-of-concept prototype that transforms any room into an immersive, augmented entertainment experience. Our system enables new interactive projection mapping experiences that dynamically adapts content to any room. Users can touch, shoot, stomp, dodge and steer projected content that seamlessly co-exists with their existing physical environment. The basic building blocks of RoomAlive are projector-depth camera units, which can be combined through a scalable, distributed framework. The projector-depth camera units are individually auto-calibrating, self-localizing, and create a unified model of the room with no user intervention. We investigate the design space of gaming experiences that are possible with RoomAlive and explore methods for dynamically mapping content based on room layout and user position. Finally we showcase four experience prototypes that demonstrate the novel interactive experiences that are possible with RoomAlive and discuss the design challenges of adapting any game to any room.},
address = {New York, New York, USA},
author = {Jones, Brett and Shapira, Lior and Sodhi, Rajinder and Murdock, Michael and Mehra, Ravish and Benko, Hrvoje and Wilson, Andrew and Ofek, Eyal and MacIntyre, Blair and Raghuvanshi, Nikunj},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/2642918.2647383},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jones et al. - 2014 - RoomAlive Magical experiences enabled by scalable, adaptive projector-camera units(2).pdf:pdf},
isbn = {9781450330695},
keywords = {Projection mapping,Projector-camera system,Spatial augmented reality},
pages = {637--644},
publisher = {Association for Computing Machinery},
series = {UIST '14},
title = {{RoomAlive: Magical Experiences Enabled by Scalable, Adaptive Projector-Camera Units}},

year = {2014}
}
@inproceedings{Chenechal2016Vishnu,
abstract = {Increasing networking performances as well as the emergence of Mixed Reality (MR) technologies make possible providing advanced interfaces to improve remote collaboration. In this paper, we present our novel interaction paradigm called Vishnu that aims to ease collaborative remote guiding. We focus on collaborative remote maintenance as an illustrative use case. It relies on an expert immersed in Virtual Reality (VR) in the remote workspace of a local agent helped through an Augmented Reality (AR) interface. The main idea of the Vishnu paradigm is to provide the local agent with two additional virtual arms controlled by the remote expert who can use them as interactive guidance tools. Many challenges come with this: collocation, inverse kinematics (IK), the perception of the remote collaborator and gestures coordination. Vishnu aims to enhance the maintenance procedure thanks to a remote expert who can show to the local agent the exact gestures and actions to perform. Our pilot user study shows that it may decrease the cognitive load compared to a usual approach based on the mapping of 2D and de-localized informations, and it could be used by agents in order to perform specific procedures without needing to have an available local expert.},
author = {{Le Ch{\'{e}}n{\'{e}}chal}, Morgan and Duval, Thierry and Gouranton, Valerie and Royan, Jerome and Arnaldi, Bruno},
booktitle = {2016 IEEE Third VR International Workshop on Collaborative Virtual Environments (3DCVE)},
doi = {10.1109/3DCVE.2016.7563559},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chenechal et al. - 2016 - Vishnu virtual immersive support for HelpiNg users an interaction paradigm for collaborative remote guiding in.pdf:pdf},
isbn = {978-1-5090-2138-3},
keywords = {AR interface,Cameras,Collaboration,H.5.1 [Information Interfaces and Presentation (e.,H.5.2 [Information Interfaces and Presentation (e.,HelpiNg users,I.3.6 [Computer Graphics]: Methodology and Techniq,Layout,MR technologies,Maintenance engineering,Navigation,Three-dimensional displays,VR,Virtual reality,Vishnu paradigm,and virtual realities,augmented,augmented reality,cognitive load,collaborative remote guiding,collaborative remote maintenance,interaction paradigm,interactive guidance tools,interactive systems,inverse kinematics,maintenance procedure,mixed reality,remote collaboration,remote collaborator,remote workspace,virtual immersive support,virtual reality},
month = {mar},
pages = {9--12},
publisher = {IEEE},
title = {{Vishnu: virtual immersive support for HelpiNg users an interaction paradigm for collaborative remote guiding in mixed reality}},

year = {2016}
}
@inproceedings{Nagai2015LiveSphere,
abstract = {Sharing an immersive experience enhances situational awareness, enabling effective collaboration between persons in different places. The development of a headworn camera enables us to capture the first-person view and share it as the personal experience. However, the limited viewing angle of such cameras prevents a remote viewer from obtaining a complete surrounding situation. In this paper, we propose a system called LiveSphere, which presents the entire surrounding visual environment using a head-worn system with multiple cameras. The imagestabilizing algorithm compensates image motion caused by the wearer's head movement. This enables the remote viewer to look around the environment independently from the wearer's head direction. We developed a prototype to examine how sharing the surrounding visual environment improves collaboration between persons in different places.},
address = {New York, New York, USA},
author = {Nagai, Shohei and Kasahara, Shunichi and Rekimoto, Jun},
booktitle = {Proceedings of the Ninth International Conference on Tangible, Embedded, and Embodied Interaction},
doi = {10.1145/2677199.2680549},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nagai, Kasahara, Rekimoto - 2015 - LiveSphere Sharing the surrounding visual environment for immersive experience in remote collaboratio.pdf:pdf},
isbn = {9781450333054},
keywords = {Experience sharing,Remote collaboration,Spherical image,Visual environment,Wearable computing},
month = {jan},
pages = {113--116},
publisher = {Association for Computing Machinery},
series = {TEI '15},
title = {{LiveSphere: Sharing the Surrounding Visual Environment for Immersive Experience in Remote Collaboration}},

year = {2015}
}
@inproceedings{Wang2019aMR,
abstract = {In this paper, we describe a new Mixed Reality (MR) remote collaborative platform making use of 3D CAD models for training in the manufacturing industry. It enables a remote expert in Virtual Reality (VR) to train a local worker in a physical assembly task. For the local site, we use Spatial Augmented Reality (SAR) to enable the local worker see virtual cues without wearing any AR devices, leaving their user hands free to easily manipulate the physical parts. For the remote expert, we construct a 3D virtual environment using virtual replicas of the physical parts. We also report on the results of a usability study of the prototype.},
author = {Wang, Peng and Bai, Xiaoliang and Billinghurst, Mark and Zhang, Shusheng and Han, Dechuan and Lv, Hao and He, Weiping and Yan, Yuxiang and Zhang, Xiangyu and Min, Haitao},
booktitle = {2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
doi = {10.1109/ISMAR-Adjunct.2019.00038},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2019 - An MR Remote Collaborative Platform Based on 3D CAD Models for Training in Industry.pdf:pdf},
isbn = {978-1-7281-4765-9},
keywords = {3D CAD models,Augmented Reality,Mixed reality,Remote collaboration,Training in the industry},
month = {oct},
pages = {91--92},
publisher = {IEEE},
title = {{An MR Remote Collaborative Platform Based on 3D CAD Models for Training in Industry}},

year = {2019}
}
@inproceedings{Piumsomboon2017Exploring,
abstract = {In this paper,we explore techniques for enhancing remote Mixed Reality (MR) collaboration in terms of communication and interaction. We created CoVAR, a MR system for remote collaboration between an Augmented Reality (AR) and Augmented Virtuality (AV) users. Awareness cues and AV-Snap-To-AR interface were proposed for enhancing communication. Collaborative natural interaction, and AV-User-Body-Scaling were implemented for enhancing interaction. We conducted an exploratory study examining the awareness cues and the collaborative gaze, and the results showed the benefits of the proposed techniques for enhancing communication and interaction.},
address = {New York, New York, USA},
author = {Piumsomboon, Thammathip and Day, Arindam and Ens, Barrett and Lee, Youngho and Lee, Gun and Billinghurst, Mark},
booktitle = {SIGGRAPH Asia 2017 Mobile Graphics \& Interactive Applications},
doi = {10.1145/3132787.3139200},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Piumsomboon et al. - 2017 - Exploring enhancements for remote mixed reality collaboration.pdf:pdf},
isbn = {9781450354103},
keywords = {Mixed reality,Remote collaboration},
month = {nov},
pages = {1--5},
publisher = {Association for Computing Machinery},
series = {SA '17},
title = {{Exploring Enhancements for Remote Mixed Reality Collaboration}},

year = {2017}
}
@inproceedings{Chen20153D,
abstract = {This paper describes a new Augmented Reality (AR) system called HoloLens developed by Microsoft, and the interaction model for supporting collaboration in this space with other users. Whereas traditional AR collaboration is between two or more head-mounted displays (HMD) users, we describe collaboration between a single HMD user and others who join the space by hitching on the view of the HMD user. The remote companions participate remotely through Skype-enabled devices such as tablets or PC's. The interaction is novel in the use of a 3D space with digital objects where the interaction by remote parties can be achieved asynchronously and reflected back to the primary user. We describe additional collaboration scenarios possible with this arrangement.},
address = {New York, New York, USA},
annote = {Remote AR with hololens enabled skype. Only a single AR user and multiple companion device(3D viewing device) users.},
author = {Chen, Henry and Lee, Austin S and Swift, Mark and Tang, John C},
booktitle = {Proceedings of the 3rd International Workshop on Immersive Media Experiences},
doi = {10.1145/2814347.2814350},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2015 - 3D collaboration method over hololens and skype end points(2).pdf:pdf},
isbn = {9781450337458},
keywords = {Asynchronous,Mixed reality,Remote collaboration},
pages = {27--30},
publisher = {Association for Computing Machinery},
series = {ImmersiveME '15},
title = {{3D Collaboration Method over HoloLens™ and Skype™ End Points}},

year = {2015}
}
@inproceedings{Kratz2015Polly,
abstract = {In this paper we report findings from two user studies that explore the problem of establishing common viewpoint in the context of a wearable telepresence system. In our first study, we assessed the ability of a local person (the guide) to identify the view orientation of the remote person by looking at the physical pose of the telepresence device. In the follow-up study, we explored visual feedback methods for communicating the relative viewpoints of the remote user and the guide via a head-mounted display. Our results show that actively observing the pose of the device is useful for viewpoint estimation. However, in the case of telepresence devices without physical directional affordances, a live video feed may yield comparable results. Lastly, more abstract visualizations lead to significantly longer recognition times, but may be necessary in more complex environments.},
address = {New York, New York, USA},
author = {Kratz, Sven and Avrahami, Daniel and Kimber, Don and Vaughan, Jim and Proppe, Patrick and Severns, Don},
booktitle = {Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
doi = {10.1145/2786567.2787134},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kratz et al. - 2015 - Polly wanna show you Examining viewpoint-conveyance techniques for a shoulder-worn telepresence system.pdf:pdf},
isbn = {9781450336536},
keywords = {HMD,Mobile,Telepresence,View orientation conveyance,Visualization},
month = {aug},
pages = {567--575},
publisher = {Association for Computing Machinery},
series = {MobileHCI '15},
title = {{Polly Wanna Show You: Examining Viewpoint-Conveyance Techniques for a Shoulder-Worn Telepresence System}},

year = {2015}
}
@inproceedings{Kato1999Marker,
abstract = {We describe an augmented reality conferencing system which uses the overlay of virtual images on the real world. Remote collaborators are represented on virtual monitors which can be freely positioned about a user in space. Users can collaboratively view and interact with virtual objects using a shared virtual whiteboard. This is possible through precise virtual image registration using fast and accurate computer vision techniques and head mounted display (HMD) calibration. We propose a method for tracking fiducial markers and a calibration method for optical see-through HMD based on the marker tracking.},
author = {Kato, Hirokazu and Billinghurst, Mark},
booktitle = {Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99)},
doi = {10.1109/IWAR.1999.803809},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kato, Billinghurst - 1999 - Marker tracking and HMD calibration for a video-based augmented reality conferencing system.pdf:pdf},
isbn = {0-7695-0359-4},
pages = {85--94},
publisher = {IEEE},
title = {{Marker tracking and HMD calibration for a video-based augmented reality conferencing system}},

year = {1999}
}
@inproceedings{Lindlbauer2018Remixed,
abstract = {We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.},
address = {New York, NY, USA},
author = {Lindlbauer, David and Wilson, Andy D},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3173574.3173703},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lindlbauer, Wilson - 2018 - Remixed reality Manipulating space and time in augmented Reality(2).pdf:pdf},
isbn = {9781450356206},
keywords = {Augmented reality,Remixed Reality,Virtual reality},
month = {apr},
pages = {1--13},
publisher = {Association for Computing Machinery},
series = {CHI '18},
title = {{Remixed Reality: Manipulating Space and Time in Augmented Reality}},

year = {2018}
}
@article{Tait2015Effect,
abstract = {In this paper we describe a system for remote collaboration using Augmented Reality (AR) that supports view independence. The system was designed to allow a remote user to assist a local user in an object placement task. The remote helper can navigate the local user's scene independently from the local user by using a three-dimensional reconstruction of the environment. The remote user can also place virtual annotations in the scene that the local user views through a head mounted display. The system tracks the position of key physical objects and the local user's head pose, displaying both to the remote user. A key advantage of this system compared to other collaborative AR interfaces is that it allows the remote expert to have an independent view into the shared task space. A user study was performed using this system with the users completing a simple mostly two-dimensional object placement task. It was designed to test how the amount of remote view independence affected collaboration. Four conditions were used with varying degrees of view independence. It was found that increased view independence led to faster task completion time, more confidence from users, and a decrease in the amount of time spent communicating verbally during the task. However, varying the view independence did not significantly change the accuracy of the placement of objects. The implications of the results are discussed along with directions for future research.},
author = {Tait, Matthew and Billinghurst, Mark},
doi = {10.1007/s10606-015-9231-8},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tait, Billinghurst - 2015 - The Effect of View Independence in a Collaborative AR System.pdf:pdf},
issn = {1573-7551},
journal = {Computer Supported Cooperative Work (CSCW)},
month = {dec},
number = {6},
pages = {563--589},
publisher = {Springer},
title = {{The Effect of View Independence in a Collaborative AR System}},

volume = {24},
year = {2015}
}
@misc{Slack,
author = {{Slack Technologies Inc.}},
title = {{Slack}},
url = {https://slack.com/},
urldate = {2021-04-09},
year = {2021}
}
@inproceedings{Lee2020Unified,
abstract = {Virtual Reality (VR) and Augmented Reality (AR) have become familiar technologies with related markets growing rapidly every year. Moreover, the idea of considering VR and AR as one eXtended reality (XR) has broken the border between virtual space and real space. However, there is no formal way to create such XR content except through existing VR or AR content development platforms. These platforms require the content author to perform additional tasks such as duplicating content for a specific user interaction environment (VR or AR) and associating them as one. Also, describing the content in an existing markup language (e.g., X3D, X3DOM, A-frame) has limitations of that the content author should predefine the user interaction environment (i.e., either of VR and AR). In this study, a unified XR representation is defined for describing XR content, and the method to render it has been proposed. The unified XR representation extends the HTML so that content authored with this representation can be harmoniously incorporated into existing web documents and can exploit resources on the World Wide Web. The XR renderer, which draws XR content on the screen, follows different procedures for both VR and AR situations. Consequently, the XR content works in both user interaction environment (VR and AR). Hence, this study provides a straightforward XR content authoring method that users access anywhere through a web browser regardless of their situational contexts, such as VR or AR. It facilitates XR collaboration with real objects by providing both VR and AR users with accessing an identical content.},
address = {New York, NY, USA},
author = {Lee, Yongjae and Moon, Changhyun and Ko, Heedong and Lee, Soo-Hong and Yoo, Byounghyun},
booktitle = {The 25th International Conference on 3D Web Technology},
doi = {10.1145/3424616.3424695},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2020 - Unified Representation for XR Content and Its Rendering Method.pdf:pdf},
isbn = {9781450381697},
keywords = {Augmented Reality,Collaboration,Content,Extended Reality,Unified Representation,Virtual Reality,XR},
month = {nov},
pages = {1--10},
publisher = {Association for Computing Machinery},
series = {Web3D '20},
title = {{Unified Representation for XR Content and Its Rendering Method}},
url = {https://dl.acm.org/doi/10.1145/3424616.3424695},
year = {2020}
}
@misc{Yohan2020Mobile,
author = {손요한},
booktitle = {Platum},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/요한 - 2020 - 모바일 협업 툴 시장 '531\%' 성장{\ldots}압도적 1위는 '줌'.pdf:pdf},
title = {모바일 협업 툴 시장 '531\%' 성장{\ldots}압도적 1위는 '줌'},
url = {https://platum.kr/archives/151840},
urldate = {2021-04-06},
year = {2020}
}
@inproceedings{Febretti2013CAVE2,
abstract = {Hybrid Reality Environments represent a new kind of visualization spaces that blur the line between virtual environments and high resolution tiled display walls. This paper outlines the design and implementation of the CAVE2TM Hybrid Reality Environment. CAVE2 is the world's first near-seamless flat-panel-based, surround-screen immersive system. Unique to CAVE2 is that it will enable users to simultaneously view both 2D and 3D information, providing more flexibility for mixed media applications. CAVE2 is a cylindrical system of 24 feet in diameter and 8 feet tall, and consists of 72 near-seamless, off-axisoptimized passive stereo LCD panels, creating an approximately 320 degree panoramic environment for displaying information at 37 Megapixels (in stereoscopic 3D) or 74 Megapixels in 2D and at a horizontal visual acuity of 20/20. Custom LCD panels with shifted polarizers were built so the images in the top and bottom rows of LCDs are optimized for vertical off-center viewing- allowing viewers to come closer to the displays while minimizing ghosting. CAVE2 is designed to support multiple operating modes. In the Fully Immersive mode, the entire room can be dedicated to one virtual simulation. In 2D model, the room can operate like a traditional tiled display wall enabling users to work with large numbers of documents at the same time. In the Hybrid mode, a mixture of both 2D and 3D applications can be simultaneously supported. The ability to treat immersive work spaces in this Hybrid way has never been achieved before, and leverages the special abilities of CAVE2 to enable researchers to seamlessly interact with large collections of 2D and 3D data. To realize this hybrid ability, we merged the Scalable Adaptive Graphics Environment (SAGE) - a system for supporting 2D tiled displays, with Omegalib - a virtual reality middleware supporting OpenGL, OpenSceneGraph and Vtk applications.},
author = {Febretti, Alessandro and Nishimoto, Arthur and Thigpen, Terrance and Talandis, Jonas and Long, Lance and Pirtle, J D and Peterka, Tom and Verlo, Alan and Brown, Maxine and Plepys, Dana and Sandin, Dan and Renambot, Luc and Johnson, Andrew and Leigh, Jason},
booktitle = {The Engineering Reality of Virtual Reality 2013},
doi = {10.1117/12.2005484},
editor = {Dolinsky, Margaret and McDowall, Ian E.},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Febretti et al. - 2013 - CAVE2 a hybrid reality environment for immersive simulation and information analysis.pdf:pdf},
month = {mar},
pages = {9--20},
publisher = {SPIE},
title = {{CAVE2: a hybrid reality environment for immersive simulation and information analysis}},

volume = {8649},
year = {2013}
}
@inproceedings{Grandi2019Characterizing,
abstract = {We present an assessment of asymmetric interactions in Collaborative Virtual Environments (CVEs). In our asymmetric setup, two co-located users interact with virtual 3D objects, one in immersive Virtual Reality (VR) and the other in mobile Augmented Reality (AR). We conducted a study with 36 participants to evaluate performance and collaboration aspects of pair work, and compare it with two symmetric scenarios, either with both users in immersive VR or mobile AR. To perform this experiment, we adopt a collaborative AR manipulation technique from literature and develop and evaluate a VR manipulation technique of our own. Our results indicate that pairs in asymmetric VR-AR achieved significantly better performance than the AR symmetric condition, and similar performance to VR symmetric. Regardless of the condition, pairs had similar work participation indicating a high cooperation level even when there is a visualization and interaction asymmetry between the participants.},
author = {Grandi, Jeronimo Gustavo and Debarba, Henrique Galvan and Maciel, Anderson},
booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
doi = {10.1109/VR.2019.8798080},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grandi, Debarba, Maciel - 2019 - Characterizing Asymmetric Collaborative Interactions in Virtual and Augmented Realities.pdf:pdf},
isbn = {978-1-7281-1377-7},
keywords = {Centered computing,Collaborative and social computing,Human,Human computer interaction (HCI),Interaction paradigms,Interaction techniques,Mixed/augmented reality human},
month = {mar},
pages = {127--135},
publisher = {IEEE},
title = {{Characterizing Asymmetric Collaborative Interactions in Virtual and Augmented Realities}},

year = {2019}
}
@inproceedings{Pauchet2007Mutual,
abstract = {Shared interface allowing several users in co-presence to interact simultaneously on digital data on a single display is an uprising challenge in Human Computer Interaction (HCI). Its development is motivated by the advent of large displays such as wall-screens and tabletops. It affords fluid and natural digital interaction without hindering human communication and collaboration. It enables mutual awareness, making participant conscious of each other activities.},
address = {Berlin, Heidelberg},
author = {Pauchet, A and Coldefy, F and Lefebvre, L and Picard, S Louis Dit and Bouguet, A and Perron, L and Guerin, J and Corvaisier, D and Collobert, M},
booktitle = {Human-Computer Interaction -- INTERACT 2007},
doi = {10.1007/978-3-540-74796-3_8},
editor = {Baranauskas, C{\'{e}}cilia and Palanque, Philippe and Abascal, Julio and Barbosa, Simone Diniz Junqueira},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pauchet et al. - 2007 - Mutual Awareness in Collocated and Distant Collaborative Tasks Using Shared Interfaces.pdf:pdf},
isbn = {978-3-540-74796-3},
pages = {59--73},
publisher = {Springer},
title = {{Mutual Awareness in Collocated and Distant Collaborative Tasks Using Shared Interfaces}},

year = {2007}
}
@inproceedings{Muller2016Virtual,
abstract = {In collaborative activities, collaborators can use physical objects in their shared environment as spatial cues to guide each other's attention. Collaborative mixed reality environments (MREs) include both physical and virtual objects. To study how virtual objects influence collaboration and whether they are used as spatial cues, we conducted a controlled lab experiment with 16 dyads. Results of our study show that collaborators favored the virtual objects as spatial cues over the physical environment and the physical objects: Collaborators used significantly less deictic gestures in favor of more disambiguous verbal references and a decreased subjective workload when virtual objects were present. This suggests adding additional virtual objects as spatial cues to MREs to improve user experience during collaborative mixed reality tasks.},
address = {New York, NY, USA},
author = {M{\"{u}}ller, Jens and R{\"{a}}dle, Roman and Reiterer, Harald},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/2858036.2858043},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/M{\"{u}}ller, R{\"{a}}dle, Reiterer - 2016 - Virtual objects as spatial cues in collaborative mixed reality environments How they shape communicatio.pdf:pdf},
isbn = {9781450333627},
keywords = {Collaboration,Mixed reality,Virtual spatial cues},
month = {may},
pages = {1245--1249},
publisher = {Association for Computing Machinery},
series = {CHI '16},
title = {{Virtual Objects as Spatial Cues in Collaborative Mixed Reality Environments: How They Shape Communication Behavior and User Task Load}},

year = {2016}
}
@techreport{Sangyeol2020Untact,
author = {한상열 and 방문영},
file = {:D\:/Dropbox (WRL)/2020.09.15 - 석사 학위논문/논문/참고/비대면 시대의 국내 XR 활용 동향.pdf:pdf},
institution = {SPRi},
pages = {4--12},
title = {{비대면 시대의 국내 XR 활용 동향}},
year = {2020}
}
@article{Schwartz2019Eyes,
abstract = {Interacting with people across large distances is important for remote work, interpersonal relationships, and entertainment. While such face-to-face interactions can be achieved using 2D video conferencing or, more recently, virtual reality (VR), telepresence systems currently distort the communication of eye contact and social gaze signals. Although methods have been proposed to redirect gaze in 2D teleconferencing situations to enable eye contact, 2D video conferencing lacks the 3D immersion of real life. To address these problems, we develop a system for face-to-face interaction in VR that focuses on reproducing photorealistic gaze and eye contact. To do this, we create a 3D virtual avatar model that can be animated by cameras mounted on a VR headset to accurately track and reproduce human gaze in VR. Our primary contributions in this work are a jointly-learnable 3D face and eyeball model that better represents gaze direction and upper facial expressions, a method for disentangling the gaze of the left and right eyes from each other and the rest of the face allowing the model to represent entirely unseen combinations of gaze and expression, and a gaze-aware model for precise animation from headset-mounted cameras. Our quantitative experiments show that our method results in higher reconstruction quality, and qualitative results show our method gives a greatly improved sense of presence for VR avatars.},
author = {Schwartz, Gabriel and Wei, Shih-En and Wang, Te-li and Lombardi, Stephen and Simon, Tomas and Saragih, Jason and Sheikh, Yaser},
doi = {10.1145/3386569.3392493},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schwartz et al. - 2019 - The Eyes Have It An Integrated Eye and Face Model for Photorealistic Facial Animation(3).pdf:pdf},
issn = {0730-0301},
journal = {ACM Transactions on Graphics},
month = {jul},
number = {4},
publisher = {Association for Computing Machinery},
title = {{The Eyes Have It: An Integrated Eye and Face Model for Photorealistic Facial Animation}},

volume = {39},
year = {2020}
}
@inproceedings{Seo2016Webizing,
abstract = {Most mixed and augmented reality (MAR) applications require the target to be specified a priori and need the same MAR applications among cooperative users. Thus, most existing MAR applications are hard to share MAR scenes or contents without prior agreement for cooperative works. To address this problem, we propose a webizing method that redesigns component relationship of MAR content using an episode as a container of user interactions for sharing life experience and activities. The MAR scene can be gradually developed by multiple participants of the episode. Our examples demonstrate MAR contents can be cooperatively produced, accumulated and consumed like Web 2.0, a digital prosumption that has proven effective in dramatic expansion of Web contents.},
address = {New York, New York, USA},
author = {Seo, Daeil and Yoo, Byounghyun E. and Ko, Heedong},
booktitle = {Proceedings of the 19th ACM Conference on Computer Supported Cooperative Work and Social Computing Companion},
doi = {10.1145/2818052.2869078},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Seo, Yoo, Ko - 2016 - Webizing mixed reality for cooperative augmentation of life experience(3).pdf:pdf;:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Seo, Yoo, Ko - 2016 - Webizing mixed reality for cooperative augmentation of life experience(4).pdf:pdf},
isbn = {9781450339506},
keywords = {Augmented reality,Cooperative work,Distributed architecture,Experience sharing,Webizing,augmented reality,cooperative work,distributed architecture,experience sharing},
month = {feb},
pages = {401--404},
publisher = {Association for Computing Machinery},
series = {CSCW '16 Companion},
title = {{Webizing Mixed Reality for Cooperative Augmentation of Life Experience}},

year = {2016}
}
@inproceedings{Piumsomboon2018MiniMe,
abstract = {We present Mini-Me, an adaptive avatar for enhancing Mixed Reality (MR) remote collaboration between a local Augmented Reality (AR) user and a remote Virtual Reality (VR) user. The Mini-Me avatar represents the VR user's gaze direction and body gestures while it transforms in size and orientation to stay within the AR user's field of view. A user study was conducted to evaluate Mini-Me in two collaborative scenarios: an asymmetric remote expert in VR assisting a local worker in AR, and a symmetric collaboration in urban planning. We found that the presence of the Mini-Me significantly improved Social Presence and the overall experience of MR collaboration.},
address = {New York, NY, USA},
author = {Piumsomboon, Thammathip and Lee, Gun A and Hart, Jonathon D and Ens, Barrett and Lindeman, Robert W and Thomas, Bruce H and Billinghurst, Mark},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3173574.3173620},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Piumsomboon et al. - 2018 - Mini-Me An Adaptive Avatar for Mixed Reality Remote Collaboration.pdf:pdf},
isbn = {9781450356206},
keywords = {augmented reality,avatar,awareness,gaze,gesture,mixed reality,redirected,remote collaboration,remote embodiment,virtual reality},
month = {apr},
pages = {1--13},
publisher = {Association for Computing Machinery},
series = {CHI '18},
title = {{Mini-Me: An Adaptive Avatar for Mixed Reality Remote Collaboration}},

year = {2018}
}
@misc{Spatial,
author = {{Spatial Systems Inc.}},
title = {{Spatial}},
url = {https://spatial.io/},
urldate = {2021-04-09},
year = {2021}
}
@inproceedings{Pereira2019Extended,
abstract = {This paper proposes the implementation of a framework for the development of collaborative extended reality (XR) applications. Using the framework, developers can focus on understanding which collaborative mechanisms they need to implement for the respective reality model application. In this paper we specifically study collaborative mechanisms around object manipulation in Virtual Reality (VR). As such, we planned a VR prototype using the proposed framework, which was used to validate the various interaction and collaboration features in VR. The gathered data from the user tests revealed that they enjoyed the experience and the collaborative mechanisms helped them work together. Furthermore, to understand whether the framework allowed for the development of XR applications, we decided to implement an augmented reality prototype as well. Afterwards, we ran an experiment with 4 VR and 3 AR users sharing the same virtual environment. The experiment was successful at allowing them to interact in real-time in the same shared environment. Therefore, the framework enables the development of XR applications that support different mixed-reality technologies.},
annote = {협업 참여는 AR/VR 모두 가능하지만 협업 대상은 VR 뿐. + 사용자평가도 했음},
author = {Pereira, Vasco and Matos, Teresa and Rodrigues, Rui and Nobrega, Rui and Jacob, Joao},
booktitle = {2019 International Conference on Graphics and Interaction (ICGI)},
doi = {10.1109/ICGI47575.2019.8955025},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pereira et al. - 2019 - Extended reality framework for remote collaborative interactions in virtual environments.pdf:pdf},
isbn = {978-1-7281-6378-9},
keywords = {Augmented Reality,Collaboration,Extended Reality,Interaction,Virtual Reality},
month = {nov},
pages = {17--24},
publisher = {IEEE},
title = {{Extended Reality Framework for Remote Collaborative Interactions in Virtual Environments}},

year = {2019}
}
@techreport{Namseok2020Cooperation,
author = {백남석 and 이수정},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/남석, 수정 - 2020 - 기업의 디지털 전환을 위한 비대면 sw 동향.pdf:pdf},
institution = {NIPA},
pages = {1--12},
title = {기업의 디지털 전환을 위한 비대면 sw 동향},
year = {2020}
}
@inproceedings{Nuernberger2016Anchoring,
abstract = {Augmented reality enhanced collaboration systems often allow users to draw 2D gesture annotations onto video feeds to help collaborators to complete physical tasks. This works well for static cameras, but for movable cameras, perspective effects cause problems when trying to render 2D annotations from a new viewpoint in 3D. In this paper, we present a new approach towards solving this problem by using gesture enhanced annotations. By first classifying which type of gesture the user drew, we show that it is possible to render annotations in 3D in a way that conforms more to the original intention of the user than with traditional methods. We first determined a generic vocabulary of important 2D gestures for remote collaboration by running an Amazon Mechanical Turk study with 88 participants. Next, we designed a novel system to automatically handle the top two 2D gesture annotations - arrows and circles. Arrows are handled by identifying their anchor points and using surface normals for better perspective rendering. For circles, we designed a novel energy function to help infer the object of interest using both 2D image cues and 3D geometric cues. Results indicate that our approach outperforms previous methods in terms of better conveying the original drawing's meaning from different viewpoints.},
author = {Nuernberger, Benjamin and Lien, Kuo-Chin and Hollerer, Tobias and Turk, Matthew},
booktitle = {2016 IEEE Virtual Reality (VR)},
doi = {10.1109/VR.2016.7504746},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nuernberger et al. - 2016 - Anchoring 2D gesture annotations in augmented reality.pdf:pdf},
isbn = {978-1-5090-0836-0},
issn = {2375-5334},
keywords = {H.5.1 [Information Interfaces and Presentation]: M,H.5.2 [Information Interfaces and Presentation]: U},
month = {mar},
pages = {247--248},
publisher = {IEEE},
title = {{Anchoring 2D gesture annotations in augmented reality}},

year = {2016}
}
@inproceedings{Kim2014Improving,
abstract = {Video conferencing is becoming more widely used in areas other than face-to-face conversation, such as sharing real world experience with remote friends or family. In this paper we explore how adding augmented visual communication cues can improve the experience of sharing remote task space and collaborating together. We developed a prototype system that allows users to share live video view of their task space taken on a Head Mounted Display (HMD) or Handheld Display (HHD), and communicate through not only voice but also using augmented pointer or annotations drawn on the shared view. To explore the effect of having such an interface for remote collaboration, we conducted a user study comparing three video-conferencing conditions with different combination of communication cues: (1) voice only, (2) voice + pointer, and (3) voice + annotation. The participants used our remote collaboration system to share a parallel experience of puzzle solving in the user study, and we found that adding augmented visual cues significantly improved the sense of being together. The pointer was the most preferred additional cue by users for parallel experience, and there were different states of the users' behavior found in remote collaboration.},
author = {Kim, Seungwon and Lee, Gun and Sakata, Nobuchika and Billinghurst, Mark},
booktitle = {2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
doi = {10.1109/ISMAR.2014.6948412},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2014 - Improving co-presence with augmented visual communication cues for sharing experience through video conference.pdf:pdf},
isbn = {978-1-4799-6184-9},
keywords = {Augmented Reality,Cameras,Collaboration,HHD,HMD,Prototypes,Streaming media,Tracking,Video Conferencing,Visual communication,Visualization,augmented reality,augmented visual communication cue,face-to-face conversation,handheld display,head mounted display,helmet mounted displays,live video sharing,parallel experience,remote task space sharing,telecommunication computing,teleconferencing,user behavior,video communication,video conference,voice only cue,voice plus annotation cue,voice plus pointer cue},
month = {sep},
pages = {83--92},
publisher = {IEEE},
title = {{Improving co-presence with augmented visual communication cues for sharing experience through video conference}},

year = {2014}
}
@inproceedings{Huang2013HandsIn3D,
abstract = {A collaboration scenario involving a remote helper guiding in real time a local worker in performing a task on physical objects is common in a wide range of industries including health, mining and manufacturing. An established ICT approach to supporting this type of collaboration is to provide a shared visual space and some form of remote gesture. The shared space and remote gesture are generally presented in a 2D video form. Recent research in tele-presence has indicated that technologies that support co-presence and immersion not only improve the process of collaboration but also improve spatial awareness of the remote participant. We therefore propose a novel approach to developing a 3D system based on a 3D shared space and 3D hand gestures. A proof of concept system for remote guidance called HandsIn3D has been developed. This system uses a head tracked stereoscopic HMD that allows the helper to be immersed in the virtual 3D space of the worker's workspace. The system captures in 3D the hands of the helper and fuses the hands into the shared workspace. This paper introduces HandsIn3D and presents a user study to demonstrate the feasibility of our approach. {\textcopyright} 2013 IFIP International Federation for Information Processing.},
annote = {AR 사용자: rgbd로 environment 미러링
VR 사용자: 헤드 트래킹을 통한 HMD 6자유도 보장 및 rgbd로 양손 미러링
AR-VR 협업},
author = {Huang, Weidong and Alem, Leila and Tecchia, Franco},
booktitle = {Human-Computer Interaction -- INTERACT 2013},
doi = {10.1007/978-3-642-40483-2_5},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang, Alem, Tecchia - 2013 - HandsIn3D Supporting remote guidance with immersive virtual environments.pdf:pdf},
isbn = {9783642404825},
issn = {03029743},
keywords = {co-presence,hand gesture,mixed reality,remote collaboration,shared visual space},
pages = {70--77},
publisher = {Springer},
title = {{HandsIn3D: Supporting Remote Guidance with Immersive Virtual Environments}},

volume = {8117},
year = {2013}
}
@inproceedings{Piumsomboon2019Shoulder,
abstract = {We propose a multi-scale Mixed Reality (MR) collaboration between the Giant, a local Augmented Reality user, and the Miniature, a remote Virtual Reality user, in Giant-Miniature Collaboration (GMC). The Miniature is immersed in a 360-video shared by the Giant who can physically manipulate the Miniature through a tangible interface, a combined 360-camera with a 6 DOF tracker. We implemented a prototype system as a proof of concept and conducted a user study (n=24) comprising of four parts comparing: A) two types of virtual representations, B) three levels of Miniature control, C) three levels of 360-video view dependencies, and D) four 360-camera placement positions on the Giant. The results show users prefer a shoulder mounted camera view, while a view frustum with a complimentary avatar is a good visualization for the Miniature virtual representation. From the results, we give design recommendations and demonstrate an example Giant-Miniature Interaction.},
address = {New York, NY, USA},
author = {Piumsomboon, Thammathip and Lee, Gun A and Irlitti, Andrew and Ens, Barrett and Thomas, Bruce H and Billinghurst, Mark},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3290605.3300458},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Piumsomboon et al. - 2019 - On the Shoulder of the Giant A Multi-Scale Mixed Reality Collaboration with 360 Video Sharing and Tangible I.pdf:pdf},
isbn = {9781450359702},
keywords = {live panorama sharing,mixed reality,multi-scale,remote collaboration,tangible user interface,wearable interface},
month = {may},
pages = {1--17},
publisher = {Association for Computing Machinery},
series = {CHI '19},
title = {{On the Shoulder of the Giant: A Multi-Scale Mixed Reality Collaboration with 360 Video Sharing and Tangible Interaction}},

year = {2019}
}
@inproceedings{Higuch2015ImmerseBoard,
abstract = {ImmerseBoard is a system for remote collaboration through a digital whiteboard that gives participants a 3D immersive experience, enabled only by an RGBD camera (Microsoft Kinect) mounted on the side of a large touch display. Using 3D processing of the depth images, life-sized rendering, and novel visualizations, ImmerseBoard emulates writing side-by-side on a physical whiteboard, or alternatively on a mirror. User studies involving three tasks show that compared to standard video conferencing with a digital whiteboard, Im-merseBoard provides participants with a quantitatively better ability to estimate their remote partners' eye gaze direction, gesture direction, intention, and level of agreement. Moreover , these quantitative capabilities translate qualitatively into a heightened sense of being together and a more enjoyable experience. ImmerseBoard's form factor is suitable for practical and easy installation in homes and offices.},
address = {New York, New York, USA},
author = {Higuchi, Keita and Chen, Yinpeng and Chou, Philip A and Zhang, Zhengyou and Liu, Zicheng},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
doi = {10.1145/2702123.2702160},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Higuch et al. - 2015 - ImmerseBoard Immersive Telepresence Experience using a Touch-Enabled Digital Whiteboard.pdf:pdf},
isbn = {9781450331456},
keywords = {a,collaboration,figure 1,immerseboard setup and conditions,immersive experience,kinect,setup including a large,telepresence,touch display and a},
pages = {2383--2392},
publisher = {Association for Computing Machinery},
series = {CHI '15},
title = {{ImmerseBoard: Immersive Telepresence Experience Using a Digital Whiteboard}},

year = {2015}
}
@misc{MOBILEINDEX2020Collaboration,
author = {MOBILEINDEX},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/MOBILEINDEX - 2020 - 협업 툴 업종 앱 사용자 현황.pdf:pdf},
title = {협업 툴 업종 앱 사용자 현황},
url = {https://hd.mobileindex.com/report/?s=138&p=3},
urldate = {2021-04-06},
year = {2020}
}
@inproceedings{Lee2017Mixed,
abstract = {One of the popular features on modern social networking platforms is sharing live 360 panorama video. .is research investigates on how to further improve shared live panorama based collaborative experiences by applying Mixed Reality (MR) technology. Shared- Sphere is a wearable MR remote collaboration system. In addition to sharing a live captured immersive panorama, SharedSphere enriches the collaboration through overlaying MR visualisation of non-verbal communication cues (e.g., view awareness and gestures cues). User feedback collected through a preliminary user study indicated that sharing of live 360 panorama video was beneficial by providing a more immersive experience and supporting view independence. Users also felt that the view awareness cues were helpful for understanding the remote collaborator's focus.},
address = {New York, New York, USA},
author = {Lee, Gun A. and Teo, Theophilus and Kim, Seungwon and Billinghurst, Mark},
booktitle = {SIGGRAPH Asia 2017 Mobile Graphics \& Interactive Applications},
doi = {10.1145/3132787.3139203},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2017 - Mixed reality collaboration through sharing a live panorama.pdf:pdf},
isbn = {9781450354103},
keywords = {Panorama,Remote collaboration,Shared experience},
month = {nov},
pages = {1--4},
publisher = {Association for Computing Machinery},
series = {SA '17},
title = {{Mixed Reality Collaboration through Sharing a Live Panorama}},

year = {2017}
}
@misc{CiscoWebex,
author = {{Cisco Systems Inc.}},
title = {{Cisco Webex}},
url = {https://www.cisco.com/c/en/us/products/conferencing/webex.html},
urldate = {2021-04-09},
year = {2021}
}
@article{MarketsandMarktets2019Enterprise,
author = {MarketsandMarkets},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/MarketsandMarkets - 2019 - Enterprise Collaboration Market by Component - Global Forecast to 2024.pdf:pdf},
title = {{Enterprise Collaboration Market by Component - Global Forecast to 2024}},
url = {https://www.marketsandmarkets.com/Market-Reports/enterprise-collaboration-market-130299553.html},
year = {2019}
}
@inproceedings{Kasahara2012Second,
abstract = {An environment for creative collaboration is significant for enhancing human communication and expressive activities, and many researchers have explored different collaborative spatial interaction technologies. However, most of these systems require special equipment and cannot adapt to everyday environment. We introduce Second Surface, a novel multi-user Augmented reality system that fosters a real-time interaction for user-generated contents on top of the physical environment. This interaction takes place in the physical surroundings of everyday objects such as trees or houses. Our system allows users to place three dimensional drawings, texts, and photos relative to such objects and share this expression with any other person who uses the same software at the same spot. Second Surface explores a vision that integrates collaborative virtual spaces into the physical space. Our system can provide an alternate reality that generates a playful and natural interaction in an everyday setup.},
address = {New York, NY, USA},
author = {Kasahara, Shunichi and Heun, Valentin and Lee, Austin S and Ishii, Hiroshi},
booktitle = {SIGGRAPH Asia 2012 Emerging Technologies},
doi = {10.1145/2407707.2407727},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kasahara et al. - 2012 - Second surface multi-user spatial collaboration system based on augmented reality.pdf:pdf},
isbn = {9781450319126},
month = {nov},
pages = {1--4},
publisher = {Association for Computing Machinery},
series = {SA '12},
title = {{Second Surface: Multi-User Spatial Collaboration System Based on Augmented Reality}},

year = {2012}
}
@inproceedings{Tecchia20123D,
abstract = {There is currently a strong need for collaborative systems with which two or more participants interact over a distance on a task involving tangible artifacts (e.g., a machine, a patient, a tool). The present paper focuses on the specific category of remote-collaboration systems where hand gestures are used by a remote helper to assist a physically distant worker to perform manual tasks. Existing systems use a combination of video capturing, 2D monitors or 2D projectors, however displaying a video of the remote workspace and allowing helpers to gesture over the video does not provide helpers with sufficient understanding of the spatial relationships between remote objects and between their hands and the remote objects. In this paper we introduce our tele-presence Mixed Reality system for remote collaboration on physical tasks based on real-time capture and rendering of the remote workspace and of the helper's hands. We improve on previous 2D systems introducing 3D capturing and rendering, and exploiting the possibility offered by the use of real 3D data to increase the feeling of immersion offered by the system using head tracking, stereoscopic rendering, inter-occlusion handling and virtual shadowing. We performed initial usability test of our system to verify if users are satisfied with the spatial awareness the system provides. {\textcopyright} 2012 ACM.},
address = {New York, New York, USA},
author = {Tecchia, Franco and Alem, Leila and Huang, Weidong},
booktitle = {Proceedings of the 11th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry},
doi = {10.1145/2407516.2407590},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tecchia, Alem, Huang - 2012 - 3D helping hands A gesture based MR system for remote collaboration.pdf:pdf},
isbn = {9781450318259},
keywords = {3D capture and rendering,Microsoft Kinect,mixed-reality,remote collaboration,tele-presence},
pages = {323--328},
publisher = {Association for Computing Machinery},
series = {VRCAI '12},
title = {{3D Helping Hands: A Gesture Based MR System for Remote Collaboration}},

year = {2012}
}
@inproceedings{Tome2019xR,
abstract = {We present a new solution to egocentric 3D body pose estimation from monocular images captured from a downward looking fish-eye camera installed on the rim of a head mounted virtual reality device. This unusual viewpoint, just 2 cm. away from the user's face, leads to images with unique visual appearance, characterized by severe self-occlusions and strong perspective distortions that result in a drastic difference in resolution between lower and upper body. Our contribution is two-fold. Firstly, we propose a new encoder-decoder architecture with a novel dual branch decoder designed specifically to account for the varying uncertainty in the 2D joint locations. Our quantitative evaluation, both on synthetic and real-world datasets, shows that our strategy leads to substantial improvements in accuracy over state of the art egocentric pose estimation approaches. Our second contribution is a new large-scale photorealistic synthetic dataset - xR-EgoPose - offering 383K frames of high quality renderings of people with a diversity of skin tones, body shapes, clothing, in a variety of backgrounds and lighting conditions, performing a range of actions. Our experiments show that the high variability in our new synthetic training corpus leads to good generalization to real world footage and to state of the art results on real world datasets with ground truth. Moreover, an evaluation on the Human3.6M benchmark shows that the performance of our method is on par with top performing approaches on the more classic problem of 3D human pose from a third person viewpoint.},
author = {Tome, Denis and Peluse, Patrick and Agapito, Lourdes and Badino, Hernan},
booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2019.00782},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tome et al. - 2019 - xR-EgoPose Egocentric 3D Human Pose From an HMD Camera.pdf:pdf},
isbn = {978-1-7281-4803-8},
month = {oct},
pages = {7727--7737},
publisher = {IEEE},
title = {{xR-EgoPose: Egocentric 3D Human Pose From an HMD Camera}},

year = {2019}
}
@misc{VIVETracker,
author = {{HTC Corporation}},
title = {{VIVE Tracker}},
url = {https://www.vive.com/eu/accessory/vive-tracker},
urldate = {2020-09-14},
year = {2020}
}
@inproceedings{Gauglitz2014InTouch,
abstract = {Augmented reality annotations and virtual scene navigation add new dimensions to remote collaboration. In this paper, we present a touchscreen interface for creating freehand drawings as worldstabilized annotations and for virtually navigating a scene reconstructed live in 3D, all in the context of live remote collaboration. Two main focuses of this work are (1) automatically inferring depth for 2D drawings in 3D space, for which we evaluate four possible alternatives, and (2) gesture-based virtual navigation designed specifically to incorporate constraints arising from partially modeled remote scenes. We evaluate these elements via qualitative user studies, which in addition provide insights regarding the design of individual visual feedback elements and the need to visualize the direction of drawings.},
address = {New York, New York, USA},
annote = {remote AR 시스템},
author = {Gauglitz, Steffen and Nuernberger, Benjamin and Turk, Matthew and H{\"{o}}llerer, Tobias},
booktitle = {Proceedings of the 20th ACM Symposium on Virtual Reality Software and Technology},
doi = {10.1145/2671015.2671016},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gauglitz et al. - 2014 - In touch with the remote world Remote collaboration with augmented reality drawings and virtual navigation.pdf:pdf},
isbn = {9781450332538},
keywords = {Augmented reality,CSCW,Depth interpretation,Gesture recognition,Telepresence,Touch,Video-mediated communication},
month = {nov},
pages = {197--205},
publisher = {Association for Computing Machinery},
series = {VRST '14},
title = {{In Touch with the Remote World: Remote Collaboration with Augmented Reality Drawings and Virtual Navigation}},

year = {2014}
}
@inproceedings{Junuzovic2012IllumiShare,
abstract = {Task and reference spaces are important communication channels for remote collaboration. However, all existing systems for sharing these spaces have an inherent weakness: they cannot share arbitrary physical and digital objects on arbitrary surfaces. We present IllumiShare, a new cost-effective, light-weight device that solves this issue. It both shares physical and digital objects on arbitrary surfaces and provides rich referential awareness. To evaluate IllumiShare, we studied pairs of children playing remotely. They used IllumiShare to share the task-reference space and Skype Video to share the person space. The study results show that IllumiShare shared the play space in a natural and seamless way. We also found that children preferred having both spaces compared to having only one. Moreover, we found that removing the task-reference space caused stronger negative disruptions to the play task and engagement level than removing the person space. Similarly, we found that adding the task-reference space resulted in stronger positive disruptions.},
address = {New York, New York, USA},
author = {Junuzovic, Sasa and Inkpen, Kori and Blank, Tom and Gupta, Anoop},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
doi = {10.1145/2207676.2208333},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Junuzovic et al. - 2012 - IllumiShare Sharing Any Surface(2).pdf:pdf},
isbn = {9781450310154},
keywords = {children,person,reference spaces,remote play,surface sharing,task,telepresence,video,video echo cancellation},
pages = {1919--1928},
publisher = {Association for Computing Machinery},
series = {CHI '12},
title = {{IllumiShare: Sharing Any Surface}},

year = {2012}
}
@inproceedings{Orts2016Holoportation,
abstract = {We present an end-to-end system for augmented and virtual reality telepresence, called Holoportation. Our system demonstrates high-quality, real-time 3D reconstructions of an entire space, including people, furniture and objects, using a set of new depth cameras. These 3D models can also be transmitted in real-time to remote users. This allows users wearing virtual or augmented reality displays to see, hear and interact with remote participants in 3D, almost as if they were present in the same physical space. From an audio-visual perspective, communicating and interacting with remote users edges closer to face-to-face communication. This paper describes the Holoportation technical system in full, its key interactive capabilities, the application scenarios it enables, and an initial qualitative study of using this new communication medium.},
address = {New York, NY, USA},
author = {Orts-Escolano, Sergio and Rhemann, Christoph and Fanello, Sean and Chang, Wayne and Kowdle, Adarsh and Degtyarev, Yury and Kim, David and Davidson, Philip L and Khamis, Sameh and Dou, Mingsong and Tankovich, Vladimir and Loop, Charles and Cai, Qin and Chou, Philip A and Mennicken, Sarah and Valentin, Julien and Pradeep, Vivek and Wang, Shenlong and Kang, Sing Bing and Kohli, Pushmeet and Lutchyn, Yuliya and Keskin, Cem and Izadi, Shahram},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
doi = {10.1145/2984511.2984517},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Orts-Escolano et al. - 2016 - Holoportation Virtual 3D Teleportation in Real-Time.pdf:pdf},
isbn = {9781450341899},
keywords = {3d capture,depth cameras,gpu,mixed reality,non-rigid reconstruction,real-time,telepresence},
month = {oct},
pages = {741--754},
publisher = {Association for Computing Machinery},
series = {UIST '16},
title = {{Holoportation: Virtual 3D Teleportation in Real-Time}},

year = {2016}
}
@article{Beck2013Immersive,
abstract = {We present a novel immersive telepresence system that allows distributed groups of users to meet in a shared virtual 3D world. Our approach is based on two coupled projection-based multi-user setups, each providing multiple users with perspectively correct stereoscopic images. At each site the users and their local interaction space are continuously captured using a cluster of registered depth and color cameras. The captured 3D information is transferred to the respective other location, where the remote participants are virtually reconstructed. We explore the use of these virtual user representations in various interaction scenarios in which local and remote users are face-to-face, side-by-side or decoupled. Initial experiments with distributed user groups indicate the mutual understanding of pointing and tracing gestures independent of whether they were performed by local or remote participants. Our users were excited about the new possibilities of jointly exploring a virtual city, where they relied on a world-in-miniature metaphor for mutual awareness of their respective locations. {\textcopyright} 2013 IEEE.},
author = {Beck, Stephan and Kunert, Andre and Kulik, Alexander and Froehlich, Bernd},
doi = {10.1109/TVCG.2013.33},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Beck et al. - 2013 - Immersive group-to-group telepresence.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {3D capture,Multi-user virtual reality,Telepresence},
month = {apr},
number = {4},
pages = {616--625},
pmid = {23428446},
publisher = {IEEE},
title = {{Immersive Group-to-Group Telepresence}},

volume = {19},
year = {2013}
}
@article{Shen2010Augmented,
abstract = {This paper presents the application of Augmented Reality (AR) to support concurrent collaborative product design among members of a multi-disciplinary team. A client/server framework has been developed to enable users in a distributed environment to carry out product design collaboratively. An intuitive interface, consisting of virtual and tangible interfaces, and a tri-layer model representation scheme have been designed and developed to support solid modeling and collaborative design activities in the AR-based environment. Using AR technology, the users wearing head-mounted devices can move about in a physical 3D space to view a part that is being designed from different angles and perspectives. The users can observe the design effects in real-time as the modifications are being made in the 3D space.},
author = {Shen, Y and Ong, S.K. and Nee, A.Y.C.},
doi = {10.1016/j.destud.2009.11.001},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shen, Ong, Nee - 2010 - Augmented reality for collaborative product design and development.pdf:pdf},
issn = {0142694X},
journal = {Design Studies},
keywords = {augmented reality,collaborative design,interface design,product design,virtual reality},
month = {mar},
number = {2},
pages = {118--145},
publisher = {Elsevier},
title = {{Augmented reality for collaborative product design and development}},

volume = {31},
year = {2010}
}
@inproceedings{Gao2018Real,
abstract = {In this study we present a Mixed-Reality based mobile remote collaboration system that enables an expert providing real-time assistance over a physical distance. By using the Google ARCore position tracking, we can integrate the keyframes captured with one external depth sensor attached to the mobile phone as one single 3D point-cloud data set to present the local physical environment into the VR world. This captured local scene is then wirelessly streamed to the remote side for the expert to view while wearing a mobile VR headset (HTC VIVE Focus). In this case, the remote expert can immerse himself/herself in the VR scene and provide guidance just as sharing the same work environment with the local worker. In addition, the remote guidance is also streamed back to the local side as an AR cue overlaid on top of the local video see-through display. Our proposed mobile remote collaboration system supports a pair of participants performing as one remote expert guiding one local worker on some physical tasks in a more natural and efficient way in a large scale work space from a distance by simulating the face-to-face co-work experience using the Mixed-Reality technique.},
address = {New York, New York, USA},
author = {Gao, Lei and Bai, Huidong and He, Weiping and Billinghurst, Mark and Lindeman, Robert W},
booktitle = {SIGGRAPH Asia 2018 Virtual \& Augmented Reality},
doi = {10.1145/3275495.3275515},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gao et al. - 2018 - Real-time visual representations for mobile mixed reality remote collaboration(2).pdf:pdf},
isbn = {9781450360289},
keywords = {Mixed Reality,Point cloud,RGB-D camera,Remote collaboration,Tele-presence},
pages = {1--2},
publisher = {Association for Computing Machinery},
series = {SA '18},
title = {{Real-Time Visual Representations for Mobile Mixed Reality Remote Collaboration}},

year = {2018}
}
@inproceedings{Chenechal2015Stretchable,
abstract = {The help of a remote expert to guide an agent in performing a physical task can be advantageous in many ways: saving time and money by avoiding travel, and thus increasing the rate of intervention. In many situations, the remote expert wishes to guide the agent by first placing him in the correct location to achieve the task. However, as the agent is not a robot, the expert can not use a location controller to place the agent. Instead, interaction techniques must enable the expert to achieve this task before physical manipulation guidance. In this paper, we propose a novel interaction technique for remote guiding based on arm gestures. First, the remote expert (using a VR setup) virtually collocates himself with the agent (using an AR setup), then controls virtual arms collocated with both users' shoulders. Second, if the expert starts to move forward to grasp a virtual object, the virtual arms start to stretch in order to keep the shoulders' collocation on the agent's side. This metaphor allows the agent to understand the direction of the expert's motion easily while preserving the naturalness of the interaction and avoiding the use of a frustum to represent the expert's head location.},
author = {{Le Ch{\'{e}}n{\'{e}}chal}, Morgan and Duval, Thierry and Gouranton, Val{\'{e}}rie and Royan, J{\'{e}}rome and Arnaldi, Bruno},
booktitle = {ICAT-EGVE 2015 - International Conference on Artificial Reality and Telexistence and Eurographics Symposium on Virtual Environments},
doi = {10.2312/egve.20151322},
editor = {Imura, Masataka and Figueroa, Pablo and Mohler, Betty},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ch{\'{e}}n{\'{e}}chal et al. - 2015 - The Stretchable Arms for Collaborative Remote Guiding.pdf:pdf},
isbn = {978-3-905674-84-2},
issn = {1727-530X},
publisher = {The Eurographics Association},
title = {{The Stretchable Arms for Collaborative Remote Guiding}},

year = {2015}
}
@misc{Zoom,
author = {{Zoom Video Communications Inc.}},
title = {{Zoom}},
url = {https://www.zoom.us/},
urldate = {2021-04-09},
year = {2021}
}
@article{lipson1998online,
abstract = {Contemporary product maintenance (including preventive services, repairs and upgrading) is becoming increasingly complex as products become more versatile and inherently complicated and as the number of available model variants multiplies. Consequently, maintenance is becoming a bottleneck in many engineering systems. This paper discusses a new online product maintenance approach based on augmented reality. According to this approach, graphical maintenance instruction and animation sequences are pre-coded (in VRML) at the design stage for typical procedures. These sequences are then transmitted upon request and virtually overlaid on the real product at the maintenance site, where and when they are needed. The instructions are conditional and adjust automatically to conditions at the maintenance site, according to input from the machine and updated knowledge at the manufacturer. This approach can alleviate much of the information overload and training required from maintenance personnel. Moreover, it can improve maintenance procedure efficiency by bringing updated expert knowledge to the field. This paper discusses the concept, function and components of the system and reports preliminary results of a non- immersive implementation.},
author = {Lipson, H and Shpitalni, M and Kimura, F and Goncharenko, I},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lipson et al. - 1998 - Online Product Maintenance by Web-Based Augmented Reality.pdf:pdf},
journal = {New Tools and Workflow for Product Development},
keywords = {augmented reality,expert systems,life cycle engineering,maintenance,remote diagnostics},
pages = {131--143},
title = {{Online Product Maintenance by Web-Based Augmented Reality}},

year = {1998}
}
@article{Kasahara2017JackIn,
abstract = {Sharing one's own immersive experience over the Internet is one of the ultimate goals of telepresence technology. In this paper, we present JackIn Head, a visual telepresence system featuring an omnidirectional wearable camera with image motion stabilization. Spherical omnidirectional video footage taken around the head of a local user is stabilized and then broadcast to others, allowing remote users to explore the immersive visual environment independently of the local user's head direction. We describe the system design of JackIn Head and report the evaluation results of real-time image stabilization and alleviation of cybersickness. Then, through an exploratory observation study, we investigate how individuals can remotely interact, communicate with, and assist each other with our system. We report our observation and analysis of inter-personal communication, demonstrating the effectiveness of our system in augmenting remote collaboration.},
annote = {작업자는 360 카메라를 이용해 상황을 전송,
원격협업자는 360 동영상 배경의 HMD를 통해 
현장의 상황을 파악하고 작업 지시

1. 배경(텔레프레젠스의 목적과 관련 기술의 진보사)
2. 문제점(미흡점, 발전방향)
3. 문제점 상세설명
4. 문제 해결을 위해 논문을 제안한다$\sim$
5. 논문 구성 소개
6. 논문의 기여 내용

citation 29},
author = {Kasahara, Shunichi and Nagai, Shohei and Rekimoto, Jun},
doi = {10.1109/TVCG.2016.2642947},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kasahara, Nagai, Rekimoto - 2017 - JackIn Head Immersive Visual Telepresence System with Omnidirectional Wearable Camera.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {First-person view,omnidirectional video,remote collaboration,telepresence,wearable camera},
month = {mar},
number = {3},
pages = {1222--1234},
publisher = {IEEE},
title = {{JackIn Head: Immersive Visual Telepresence System with Omnidirectional Wearable Camera}},

volume = {23},
year = {2017}
}
@article{Poretski2018Normative,
abstract = {Novel collaborative technologies afford new modes of behavior, which are often not regulated by established social norms. In particular, shared augmented reality (AR) - where multiple users can create, attach, and interact with the same virtual elements embedded into the physical environment – has the potential to interrupt current social norms of behavior. The objective of our study is to shed light on the ways in which shared AR challenges existing behavioral expectations. Using a simuated lab experimental design, we performed a study of users' interactions in a shared AR setting. Content analysis of participants' interviews reveals users' concerns over the preservation of their self- and social identity, as well as concerns related to personal space and the sense of psychological ownership over one's body and belongings. Our findings also point to the need for regulation of shared AR spaces and design of the technology's control mechanisms.},
author = {Poretski, Lev and Lanir, Joel and Arazy, Ofer},
doi = {10.1145/3274411},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Poretski, Lanir, Arazy - 2018 - Normative tensions in shared augmented reality.pdf:pdf},
issn = {2573-0142},
journal = {Proceedings of the ACM on Human-Computer Interaction},
keywords = {Augmented reality,Identity,Norms,Psychological ownership,Shared AR},
month = {nov},
number = {CSCW},
pages = {1--22},
publisher = {Association for Computing Machinery},
title = {{Normative Tensions in Shared Augmented Reality}},

volume = {2},
year = {2018}
}
@inproceedings{Huh2019XR,
abstract = {The web has been an extremely effective collaboration platform, enabling services like Wikipedia article co-authoring, blogging, social messaging, video conferencing, and many others. However, the collaboration should ideally occur Peer to Peer (P2P) among the participants instead of going through a centralized server as in the current centralized web, which acts as a mediator as well as a repository of data, especially for face-to-face collaboration in 3D XR context. Most notable XR applications like MMORPG have been developed in a dedicated application platform with their own centralized game servers. Nowadays, the decentralized web is being promoted as the next web architecture in numerous fronts such as blockchain in cryptocurrency, reviving P2P storage, and networking technologies as the next web, Web 3.0. It would be beneficial if we could make an XR collaboration framework based on the recent developments of the decentralized web. This paper explores one possible amalgamation of the decentralized web technology stack toward a webized XR collaboration framework.},
address = {New York, NY, USA},
author = {Huh, Seungyeon and Muralidharan, Shapna and Ko, Heedong and Yoo, Byounghyun},
booktitle = {The 24th International Conference on 3D Web Technology},
doi = {10.1145/3329714.3338137},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huh et al. - 2019 - XR collaboration architecture based on decentralized web.pdf:pdf},
isbn = {9781450367981},
keywords = {Collaboration,Cross reality,Decentralized web,Extended reality,Webizing,XR},
month = {jul},
pages = {1--9},
publisher = {Association for Computing Machinery},
series = {Web3D '19},
title = {{XR Collaboration Architecture Based on Decentralized Web}},

year = {2019}
}
@article{Irlitti2019Conveying,
abstract = {Spatial Augmented Reality (SAR) systems can be suitably combined with other existing Extended Reality (xR) technologies to support collaboration. In existing strategies, users unencumbered by a viewing technology, such as a tablet interface or a head-mounted display, must rely on the transmission of their collaborators' positioning through interpreting a first-person camera view. This design creates a seam between a user's experience of the augmented physical environment in SAR, and their collaborators' experience inside the virtual environment. To assist in development and evaluation of spatial cues to support spatial awareness in SAR environments, an egocentric spatial-communication taxonomy is presented given two determining dimensions, a cue's attachment (physical/virtual) and animation (local/world). We developed four egocentric cues which characterize the four independent dimensions of the matrix: arrow, path, glow, and radial, and a single exocentric world in miniature visualization. Our study shows that virtual attachment cues are preferred, providing the highest accuracy, highest performance when collaborators are occluded, and produce the least mental effort when used with a single virtual collaborator. For multiple collaborators however, the virtual attached, world animated radial cue produces significant increases in mental load and reductions in preference, demonstrating the impact of visual augmentation clutter. The single exocentric visualization produced higher levels of head movement, and poorer accuracy, however the novelty of the visualization produced positive qualitative results.},
annote = {1. 논문의 목적 간략 제시 및 배경, 문제점, 대안 방향
2. 또다른 개념 소개, 문제점, 현 연구들의 접근방법 소개
3. 본 논문이 생각하는 좋은 접근 방법 제시, 예시를 들어 이 논문의 접근방법의 유용성 설명, 여전히 남은 미흡점 소개 및 해결방안 제시
4. 논문의 기여 내용

citation 45},
author = {Irlitti, Andrew and Piumsomboon, Thammathip and Jackson, Daniel and Thomas, Bruce H},
doi = {10.1109/TVCG.2019.2932173},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Irlitti et al. - 2019 - Conveying spatial awareness cues in xR collaborations.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Animation,Collaboration,Extended reality,SAR environments,Spatial augmented reality,Stakeholders,Taxonomy,Virtual environments,Visualization,augmented physical environment,augmented reality,awareness,collaboration,data visualisation,determining dimensions,egocentric cues,extended reality technologies,first-person camera view,head-mounted display,helmet mounted displays,independent dimensions,multiple collaborators,single exocentric visualization,single exocentric world,single virtual collaborator,spatial augmented reality,spatial augmented reality systems,spatial awareness cues,spatial cues,spatial-communication taxonomy,tablet interface,viewing technology,virtual attachment cues,virtual environment,virtual reality,visual augmentation clutter,world animated radial cue,xR collaborations},
month = {nov},
number = {11},
pages = {3178--3189},
publisher = {IEEE},
title = {{Conveying spatial awareness cues in xR collaborations}},

volume = {25},
year = {2019}
}
@techreport{MarketsandMarktets2018Enterprise,
author = {MarketsandMarkets},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/MarketsandMarkets - 2018 - Enterprise Collaboration Market by Component - Global Forecast to 2023.pdf:pdf},
pages = {161},
title = {{Enterprise Collaboration Market by Component - Global Forecast to 2023}},
url = {https://www.reportsnreports.com/reports/276614-enterprise-collaboration-market-solutions-telephony-unified-messaging-conferencing-collaboration-platforms-enterprise-social-services-deployment-user-types-sme-enterprises-global-advancements-worldwide-foreca},
year = {2018}
}
@inproceedings{Teo2019Mixed,
abstract = {Remote Collaboration using Virtual Reality (VR) and Augmented Reality (AR) has recently become a popular way for people from different places to work together. Local workers can collaborate with remote helpers by sharing 360-degree live video or 3D virtual reconstruction of their surroundings. However, each of these techniques has benefits and drawbacks. In this paper we explore mixing 360 video and 3D reconstruction together for remote collaboration, by preserving benefits of both systems while reducing drawbacks of each. We developed a hybrid prototype and conducted user study to compare benefits and problems of using 360 or 3D alone to clarify the needs for mixing the two, and also to evaluate the prototype system. We found participants performed significantly better on collaborative search tasks in 360 and felt higher social presence, yet 3D also showed potential to complement. Participant feedback collected after trying our hybrid system provided directions for improvement.},
address = {New York, NY, USA},
author = {Teo, Theophilus and Lawrence, Louise and Lee, Gun A and Billinghurst, Mark and Adcock, Matt},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3290605.3300431},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Teo et al. - 2019 - Mixed reality remote collaboration combining 360 video and 3D reconstruction(2).pdf:pdf},
isbn = {9781450359702},
keywords = {360 panorama,3D scene reconstruction,Interaction methods,Mixed reality,Remote collaboration,Virtual reality},
month = {may},
pages = {1--14},
publisher = {Association for Computing Machinery},
series = {CHI '19},
title = {{Mixed Reality Remote Collaboration Combining 360 Video and 3D Reconstruction}},

year = {2019}
}
@misc{Myoungkwan2021Virnect,
author = {권명관},
booktitle = {아이티동아},
title = {버넥트, 한화토탈에 '버넥트 리모트' 공급},
url = {https://it.donga.com/30638/},
urldate = {2021-04-12},
year = {2021}
}
@misc{Myoungseok2021Daewoo,
author = {채명석},
booktitle = {서울와이어},
title = {대우조선해양, 조선업 현장에 언택트 기술 적용 성공},
url = {http://www.seoulwire.com/news/articleView.html?idxno=416106},
urldate = {2021-04-12},
year = {2021}
}
@misc{Fink2019Enterprise,
author = {Fink, Charlie},
booktitle = {Forbes},
month = {feb},
title = {{Enterprise AR Use Cases}},
url = {https://www.forbes.com/sites/charliefink/2019/02/26/enterprise-ar-use-cases/},
urldate = {2021-04-12},
year = {2019}
}
@misc{ScopeAR,
author = {{Scope Technologies US Inc.}},
title = {{Remote Assistance}},
url = {https://www.scopear.com/},
urldate = {2021-04-13},
year = {2021}
}
@misc{VirnectRemote,
author = {{VIRNECT Co. LTD.}},
title = {{VIRNECT Remote}},
url = {https://www.virnect.com/},
urldate = {2021-04-13},
year = {2021}
}

@article{Lee2021XR,
abstract = {Collaborating in a physically remote location saves time and money. Many remote collaboration systems have been studied and commercialized. Their capabilities have been confined to virtual objects and information. More recent studies have focused on collaborating in a physical environment and with physical objects. However, they have limitations including shaky and unstable views (scenes), view dependence, low scalability, and poor content expression. In this paper, we propose a web-based extended reality (XR) collaboration system that alleviates the aforementioned issues and enables effective, reproducible cooperation. Our proposed system comprises three parts: interaction device webization, which expands the web browser's device interfaces; unified XR representation, which describes content interoperable in both virtual reality (VR) and augmented reality (AR); and unified coordinate creation, which enables presenting physical objects' pose in world coordinates. With this system, a user in VR can intuitively instruct the manipulation of a physical object by manipulating a virtual object representative of the physical object. Conversely, a user in AR can catch up with the instruction by observing the augmented virtual object on the physical object. Moreover, as the pose of the physical object at the AR user's worksite is reflected in the virtual object, the VR user can recognize the working progress and give feedback to the AR user. To improve remote collaboration, we surveyed XR collaboration studies and proposed a new method for classifying XR collaborative applications based on the virtual–real engagement and ubiquitous computing continuum. We implemented a prototype and conducted a survey among submarine crews, most of whom were positively inclined to use our system, to convey that the system would be helpful in improving their job performance. Furthermore, we suggested possible improvements to it to enhance each participant's understanding of the other user's context within the XR collaboration.},
annote = {qwab012},
author = {Lee, Yongjae and Yoo, Byounghyun},
doi = {10.1093/jcde/qwab012},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Yoo - 2021 - XR collaboration beyond virtual reality work in the real world.pdf:pdf},
issn = {2288-5048},
journal = {Journal of Computational Design and Engineering},
pages = {1--17},
title = {{XR collaboration beyond virtual reality: work in the real world}},

year = {2021}
}
@techreport{Gwangki2018Digital,
author = {이광기 and 유호동 and 김탁곤},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/이광기, 유호동, 김탁곤 - 2018 - 디지털 트윈 기술 발전방향.pdf:pdf},
institution = {KEIT},
pages = {75--97},
title = {디지털 트윈 기술 발전방향},
year = {2018}
}
@misc{OpenCV2021CameraCalibration,
author = {OpenCV dev team},
title = {{Camera Calibration and 3D Reconstruction}},
url = {https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html},
urldate = {2021-04-15},
year = {2021}
}
@misc{Apple2021intrinsics,
author = {{Apple Inc.}},
title = {intrinsics},
url = {https://developer.apple.com/documentation/arkit/arcamera/2875730-intrinsics},
urldate = {2021-04-15},
year = {2021}
}
@misc{Apple2021ARDepthData,
author = {{Apple Inc.}},
title = {{ARDepthData}},
url = {https://developer.apple.com/documentation/arkit/ardepthdata},
urldate = {2021-04-15},
year = {2021}
}
@misc{MDN2021Signaling,
author = {{MDN Web Docs}},
title = {{Signaling and video calling}},
url = {https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API/Signaling_and_video_calling},
urldate = {2021-04-15},
year = {2021}
}
@misc{MDN2021IntroductionWebRTC,
author = {{MDN Web Docs}},
title = {{Introduction to WebRTC protocols}},
url = {https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API/Protocols},
urldate = {2021-04-15},
year = {2021}
}
@misc{Ball2021Wikimedia,
author = {{Wikimedia Commons}},
title = {{Ball Valve}},
url = {https://commons.wikimedia.org/w/index.php?curid=41064308},
urldate = {2021-04-16},
year = {2021}
}
@inproceedings{peng2019pvnet,
abstract = {This paper addresses the challenge of 6DoF pose estimation from a single RGB image under severe occlusion or truncation. Many recent works have shown that a two-stage approach, which first detects keypoints and then solves a Perspective-n-Point (PnP) problem for pose estimation, achieves remarkable performance. However, most of these methods only localize a set of sparse keypoints by regressing their image coordinates or heatmaps, which are sensitive to occlusion and truncation. Instead, we introduce a Pixel-wise Voting Network (PVNet) to regress pixel-wise vectors pointing to the keypoints and use these vectors to vote for keypoint locations. This creates a flexible representation for localizing occluded or truncated keypoints. Another important feature of this representation is that it provides uncertainties of keypoint locations that can be further leveraged by the PnP solver. Experiments show that the proposed approach outperforms the state of the art on the LINEMOD, Occlusion LINEMOD and YCB-Video datasets by a large margin, while being efficient for real-time pose estimation. We further create a Truncation LINEMOD dataset to validate the robustness of our approach against truncation. The code is available at https://zju3dv.github.io/pvnet/.},
author = {Peng, Sida and Liu, Yuan and Huang, Qixing and Zhou, Xiaowei and Bao, Hujun},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peng et al. - 2019 - PVNet Pixel-wise Voting Network for 6DoF Pose Estimation.pdf:pdf},
title = {{PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation}},
year = {2019}
}
@inproceedings{BOP2020,
abstract = {This paper presents the evaluation methodology, datasets, and results of the BOP Challenge 2020, the third in a series of public competitions organized with the goal to capture the status quo in the field of 6D object pose estimation from an RGB-D image. In 2020, to reduce the domain gap between synthetic training and real test RGB images, the participants were 350K photorealistic training images generated by BlenderProc4BOP, a new open-source and light-weight physically-based renderer (PBR) and procedural data generator. Methods based on deep neural networks have finally caught up with methods based on point pair features, which were dominating previous editions of the challenge. Although the top-performing methods rely on RGB-D image channels, strong results were achieved when only RGB channels were used at both training and test time -- out of the 26 evaluated methods, the third method was trained on RGB channels of PBR and real images, while the fifth on RGB channels of PBR images only. Strong data augmentation was identified as a key component of the top-performing CosyPose method, and the photorealism of PBR images was demonstrated effective despite the augmentation. The online evaluation system stays open and is available on the project website: bop.felk.cvut.cz.},
address = {Cham},
author = {Hoda\vn, Tom{\'{a}}{\v{s}} and Sundermeyer, Martin and Drost, Bertram and Labb{\'{e}}, Yann and Brachmann, Eric and Michel, Frank and Rother, Carsten and Matas, Ji\vr{\'{i}}},
booktitle = {Computer Vision -- ECCV 2020 Workshops},
editor = {Bartoli, Adrien and Fusiello, Andrea},
isbn = {978-3-030-66096-3},
pages = {577--594},
publisher = {Springer International Publishing},
title = {{BOP Challenge 2020 on 6D Object Localization}},
year = {2020}
}
@inproceedings{Wang20206PACK,
abstract = {We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.},
author = {Wang, C and Mart{\'{i}}n-Mart{\'{i}}n, R and Xu, D and Lv, J and Lu, C and Fei-Fei, L and Savarese, S and Zhu, Y},
booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA40945.2020.9196679},
file = {:C\:/Users/Lee/Downloads/09196679.pdf:pdf},
issn = {2577-087X},
keywords = {Pose estimation,Real-time systems,Robots,Robustness,Three-dimensional displays,Tracking,Visualization},
pages = {10059--10066},
title = {{6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints}},
year = {2020}
}
@inproceedings{Adcock2013RemoteFusion,
abstract = {Remote guidance systems allow humans to collaborate on physical tasks across large distances and have applications in fields such as medicine, maintenance and working with hazardous substances. Existing systems typically provide two dimensional video streams to remote participants, and these are restricted to viewpoint locations based on the placement of physical cameras. Recent systems have incorporated the ability of a remote expert to annotate their 2D view and for these annotations to be displayed in the physical workspace to the local worker. We present a prototype remote guidance system, called RemoteFusion, which is based on the volumetric fusion of commodity depth cameras. The system incorporates real-time 3D fusion with color, the ability to distinguish and render dynamic elements of a scene whether human or non-human, a multi-touch driven free 3D viewpoint, and a Spatial Augmented Reality (SAR) light annotation mechanism. We provide a physical overview of the system, including hardware and software configuration, and detail the implementation of each of the key features.},
address = {New York, New York, USA},
author = {Adcock, Matt and Anderson, Stuart and Thomas, Bruce},
booktitle = {Proceedings of the 12th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry},
doi = {10.1145/2534329.2534331},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Adcock, Anderson, Thomas - 2013 - RemoteFusion Real Time Depth Camera Fusion for Remote Collaboration on Physical Tasks Matt.pdf:pdf},
isbn = {9781450325905},
keywords = {fusion,remote guidance,spatial augmented reality,spatial user interfaces,visualization},
mendeley-groups = {2020 JCDE (XR Collabo System),2021 Thesis},
pages = {235--242},
publisher = {Association for Computing Machinery},
series = {VRCAI '13},
title = {{RemoteFusion: Real Time Depth Camera Fusion for Remote Collaboration on Physical Tasks}},

year = {2013}
}
@article{Alem2011Study,
abstract = {This paper presents the results of an experimental investigation of two gesture representations (overlaying hands and cursor pointer) in a video-mediated scenario—remote collaboration on physical task. Our study assessed the relative value of the two gesture representations with respect to their effectiveness in task performance, user's satisfaction, and user's perceived quality of collaboration in terms of the coordination and interaction with the remote partner. Our results show no clear difference between these two gesture representations in the effectiveness and user satisfaction. However, when considering the perceived quality of collaboration, the overlaying hands condition was statistically significantly higher than the pointer cursor condition. Our results seem to suggest that the value of a more expressive gesture representation is not so much a gain in performance but rather a gain in user's experience, more specifically in user's perceived quality of collaborative effort.},
author = {Alem, Leila and Li, Jane},
doi = {10.1155/2011/987830},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alem, Li - 2011 - A study of gestures in a video-mediated collaborative assembly task.pdf:pdf},
issn = {1687-5893},
journal = {Advances in Human-Computer Interaction},
mendeley-groups = {2020 JCDE (XR Collabo System)},
pages = {1--7},
publisher = {Hindawi},
title = {{A Study of Gestures in a Video-Mediated Collaborative Assembly Task}},

volume = {2011},
year = {2011}
}
@article{Aliev2019Neural,
abstract = {We present a new point-based approach for modeling the appearance of real scenes. The approach uses a raw point cloud as the geometric representation of a scene, and augments each point with a learnable neural descriptor that encodes local geometry and appearance. A deep rendering network is learned in parallel with the descriptors, so that new views of the scene can be obtained by passing the rasterizations of a point cloud from new viewpoints through this network. The input rasterizations use the learned descriptors as point pseudo-colors. We show that the proposed approach can be used for modeling complex scenes and obtaining their photorealistic views, while avoiding explicit surface estimation and meshing. In particular, compelling results are obtained for scene scanned using hand-held commodity RGB-D sensors as well as standard RGB cameras even in the presence of objects that are challenging for standard mesh-based modeling.},
archivePrefix = {arXiv},
arxivId = {1906.08240},
author = {Aliev, Kara-Ali and Sevastopolsky, Artem and Kolos, Maria and Ulyanov, Dmitry and Lempitsky, Victor},
eprint = {1906.08240},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aliev et al. - 2020 - Neural Point-Based Graphics.pdf:pdf},
keywords = {Image-based rendering,convolutional networks,neural rendering,scene modeling},
mendeley-groups = {2020 JCDE (XR Collabo System)},
month = {jun},
title = {{Neural Point-Based Graphics}},

year = {2020}
}
@inproceedings{Aschenbrenner2018Exploration,
abstract = {Figure 1: Cooperative production line planning with AR co-location ABSTRACT Travel costs and time consumption play an important role in industrial factory planning; thus many companies intend to use remote assistance and virtual co-location of the workers via software instead. Shared visual context can help in remote assistance scenarios and mixed reality approaches promise to transport the local situation to the external expert in order to give him or her the experience of 'actually being there' (remote presence). A key question for this issue is: How could the local situation be visualized to help the remote expert? Therefore, this paper investigates three different visualization methods for the remote perspective in a collaborative factory planning task. Two local operators collaborate in a shared augmented reality setting and are supervised by a third, virtually co-located person with the help of his/her interface. In a user study with 30 participants, we found significant indications that the use of a 'god-mode' perspective for the remote expert provides the best situation awareness, even compared to a 'real-size' version setting or a 'first person' perspective through the HMD (head-mounted display) camera of one of the local operators.},
author = {Aschenbrenner, Doris and Li, Meng and Dukalski, Radoslaw and {Casper Verlinden}, Jouke and Dukalski, Radoslaw and Verlinden, Jouke and Lukosch, Stephan},
booktitle = {Fourth IEEE VR International Workshop on 3D Collaborative Virtual Environments (3DCVE 2018)},
doi = {10.13140/RG.2.2.14819.66083},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aschenbrenner et al. - 2018 - Exploration of different Augmented Reality Visualizations for Enhancing Situation Awareness for Remote Fac.pdf:pdf},
keywords = {-[Computer systems organization]: Robotics-,-[Social and professional topics]: Computer suppor,Applied computing [Industry and manufactur-ing]:,Index Terms: Human-centered computing [Mixed / aug},
mendeley-groups = {2020 JCDE (XR Collabo System),2021 Thesis},
pages = {3--7},
title = {{Exploration of different Augmented Reality Visualizations for Enhancing Situation Awareness for Remote Factory Planning Assistance}},
year = {2018}
}
@inproceedings{Dey2017Effects,
abstract = {Interfaces for collaborative tasks, such as multiplayer games can enable more effective and enjoyable collaboration. However, in these systems the emotional states of the users are often not communicated properly due to their remoteness from one another. In this paper we investigate the effects of showing emotional states of one collaborator to the other during an immersive Virtual Reality (VR) gameplay experience. We created two collaborative immersive VR games that display the real-time heart-rate of one player to the other. The two different games elicited different emotions, one joyous and the other scary. We tested the effects of visualizing heart-rate feedback in comparison with conditions where such a feedback was absent. The games had significant main effects on the overall emotional experience.},
address = {New York, NY, USA},
author = {Dey, Arindam and Piumsomboon, Thammathip and Lee, Youngho and Billinghurst, Mark},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
doi = {10.1145/3025453.3026028},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dey et al. - 2017 - Effects of Sharing Physiological States of Players in a Collaborative Virtual Reality Gameplay.pdf:pdf},
isbn = {9781450346559},
keywords = {Collaborative gameplay,Emotions,Empathic computing,Physiological sensors,User study,Virtual reality},
mendeley-groups = {2020 JCDE (XR Collabo System)},
month = {may},
pages = {4045--4056},
publisher = {Association for Computing Machinery},
series = {CHI '17},
title = {{Effects of Sharing Physiological States of Players in a Collaborative Virtual Reality Gameplay}},

year = {2017}
}
@inproceedings{Lee2018User,
abstract = {Sharing and watching live 360 panorama video is available on modern social networking platforms, yet the communication is often a passive one-directional experience. This research investigates how to further improve live 360 panorama based remote collaborative experiences by adding Mixed Reality (MR) cues. SharedSphere is a wearable MR remote collaboration system that enriches a live captured immersive panorama based collaboration through MR visualisation of non-verbal communication cues (e.g., view awareness and gestures cues). We describe the design and implementation details of the prototype system, and report on a user study investigating how MR live panorama sharing affects the user's collaborative experience. The results showed that providing view independence through sharing live panorama enhances co-presence in collaboration, and the MR cues help users understanding each other. Based on the study results we discuss design implications and future research direction.},
annote = {related work 문헌 참고로 좋을 듯
2.1 Live 360 Panorama Sharing
2.2 Mixed Reality Remote Collaboration},
author = {Lee, Gun A. and Teo, Theophilus and Kim, Seungwon and Billinghurst, Mark},
booktitle = {2018 IEEE InterWebizing collaborativenational Symposium on Mixed and Augmented Reality (ISMAR)},
doi = {10.1109/ISMAR.2018.00051},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2019 - A User Study on MR Remote Collaboration Using Live 360 Video.pdf:pdf},
isbn = {978-1-5386-7459-8},
keywords = {Augmented Reality,Mixed Reality,live panorama sharing,remote collaboration,view independence},
mendeley-groups = {XR Collabo with BG,2020 Web3D (Unified XR Content Representation),2020 JCDE (XR Collabo System)},
month = {oct},
pages = {153--164},
publisher = {IEEE},
title = {{A User Study on MR Remote Collaboration Using Live 360 Video}},

year = {2018}
}
@article{Petit2010Multicamera,
abstract = {We present a multicamera real-time 3D modeling system that aims at enabling new immersive and interactive environments. This system, called Grimage, allows to retrieve in real-time a 3D mesh of the observed scene as well as the associated textures. This information enables a strong visual presence of the user into virtual worlds. The 3D shape information is also used to compute collisions and reaction forces with virtual objects, enforcing the mechanical presence of the user in the virtual world. The innovation is a fully integrated system with both immersive and interactive capabilities. It embeds a parallel version of the EPVH modeling algorithm inside a distributed vision pipeline. It also adopts the hierarchical component approach of the FlowVR middleware to enforce software modularity and enable distributed executions. Results show high refresh rates and low latencies obtained by taking advantage of the I/O and computing resources of PC clusters. The applications we have developed demonstrate the quality of the visual and mechanical presence with a single platform and with a dual platform that allows telecollaboration.},
author = {Petit, Benjamin and Lesage, Jean-Denis and Menier, Cl{\'{e}}ment and Allard, J{\'{e}}r{\'{e}}mie and Franco, Jean-S{\'{e}}bastien and Raffin, Bruno and Boyer, Edmond and Faure, Fran{\c{c}}ois},
doi = {10.1155/2010/247108},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Petit et al. - 2010 - Multicamera real-time 3D modeling for telepresence and remote collaboration(2).pdf:pdf},
issn = {1687-7578},
journal = {International Journal of Digital Multimedia Broadcasting},
mendeley-groups = {2020 JCDE (XR Collabo System)},
pages = {1--12},
publisher = {Hindawi},
title = {{Multicamera Real-Time 3D Modeling for Telepresence and Remote Collaboration}},

volume = {2010},
year = {2010}
}
@misc{MDN2021WebRTCConnectivity,
author = {{MDN Web Docs}},
mendeley-groups = {2021 Thesis},
title = {{WebRTC connectivity}},
url = {https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API/Connectivity},
urldate = {2021-04-18},
year = {2021}
}

@inproceedings{Erboz2017HowTo,
author = {Erboz, Gizem},
booktitle = {Managerial trends in the development of enterprises in globalization era},
month = {nov},
title = {{How To Define Industry 4.0: Main Pillars Of Industry 4.0}},
url = {https://www.researchgate.net/publication/326557388_How_To_Define_Industry_40_Main_Pillars_Of_Industry_40},
year = {2017}
}

@article{Negri2017Review,
abstract = {The Digital Twin (DT) is one of the main concepts associated to the Industry 4.0 wave. This term is more and more used in industry and research initiatives; however, the scientific literature does not provide a unique definition of this concept. The paper aims at analyzing the definitions of the DT concept in scientific literature, retracing it from the initial conceptualization in the aerospace field, to the most recent interpretations in the manufacturing domain and more specifically in Industry 4.0 and smart manufacturing research. DT provides virtual representations of systems along their lifecycle. Optimizations and decisions making would then rely on the same data that are updated in real-time with the physical system, through synchronization enabled by sensors. The paper also proposes the definition of DT for Industry 4.0 manufacturing, elaborated by the European H2020 project MAYA, as a contribution to the research discussion about DT concept.},
annote = {27th International Conference on Flexible Automation and Intelligent Manufacturing, FAIM2017, 27-30 June 2017, Modena, Italy},
author = {Negri, Elisa and Fumagalli, Luca and Macchi, Marco},
doi = {https://doi.org/10.1016/j.promfg.2017.07.198},
file = {:C\:/Users/Lee/Downloads/1-s2.0-S2351978917304067-main.pdf:pdf},
issn = {2351-9789},
journal = {27th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2017)},
keywords = {Cyber-Physical Systems,Digital Twin,Industry 4.0,Production Systems},
mendeley-groups = {2021 Thesis},
pages = {939--948},
publisher = {Elsevier},
title = {{A Review of the Roles of Digital Twin in CPS-based Production Systems}},
url = {https://www.sciencedirect.com/science/article/pii/S2351978917304067},
volume = {11},
year = {2017}
}

@article{Jones2020Characterising,
abstract = {While there has been a recent growth of interest in the Digital Twin, a variety of definitions employed across industry and academia remain. There is a need to consolidate research such to maintain a common understanding of the topic and ensure future research efforts are to be based on solid foundations. Through a systematic literature review and a thematic analysis of 92 Digital Twin publications from the last ten years, this paper provides a characterisation of the Digital Twin, identification of gaps in knowledge, and required areas of future research. In characterising the Digital Twin, the state of the concept, key terminology, and associated processes are identified, discussed, and consolidated to produce 13 characteristics (Physical Entity/Twin; Virtual Entity/Twin; Physical Environment; Virtual Environment; State; Realisation; Metrology; Twinning; Twinning Rate; Physical-to-Virtual Connection/Twinning; Virtual-to-Physical Connection/Twinning; Physical Processes; and Virtual Processes) and a complete framework of the Digital Twin and its process of operation. Following this characterisation, seven knowledge gaps and topics for future research focus are identified: Perceived Benefits; Digital Twin across the Product Life-Cycle; Use-Cases; Technical Implementations; Levels of Fidelity; Data Ownership; and Integration between Virtual Entities; each of which are required to realise the Digital Twin.},
author = {Jones, David and Snider, Chris and Nassehi, Aydin and Yon, Jason and Hicks, Ben},
doi = {https://doi.org/10.1016/j.cirpj.2020.02.002},
file = {:C\:/Users/Lee/Downloads/1-s2.0-S1755581720300110-main.pdf:pdf},
issn = {1755-5817},
journal = {CIRP Journal of Manufacturing Science and Technology},
keywords = {Digital Twin,Virtual Twin},
mendeley-groups = {2021 Thesis},
pages = {36--52},
title = {{Characterising the Digital Twin: A systematic literature review}},
url = {https://www.sciencedirect.com/science/article/pii/S1755581720300110},
volume = {29},
year = {2020}
}

@techreport{Shafto2010DRAFT,
author = {Shafto, Mike and Rich, Mike Conroy and Glaessgen, Doyle Ed and Kemp, Chris and Lemoigne, Jacqueline and Wang, Lui},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shafto et al. - 2010 - DRAFT MoDeling, SiMulATion, inFoRMATion Technology & PRoceSSing RoADMAP Technology Area 11.pdf:pdf},
mendeley-groups = {2021 Thesis},
pages = {27},
title = {{DRAFT MoDeling, SiMulATion, inFoRMATion Technology \& PRoceSSing RoADMAP Technology Area 11}},
url = {https://www.nasa.gov/pdf/501321main_TA11-MSITP-DRAFT-Nov2010-A1.pdf},
year = {2010}
}

@techreport{Shafto2012Modeling,
author = {Shafto, Mike and Rich, Mike Conroy and Glaessgen, Doyle Ed and Kemp, Chris and Lemoigne, Jacqueline and Wang, Lui},
mendeley-groups = {2021 Thesis},
title = {{Modeling, SiMulation, inforMation technology \& ProceSSing roadMaP Technology Area 11}},
url = {https://www.nasa.gov/sites/default/files/501321main_TA11-ID_rev4_NRC-wTASR.pdf},
year = {2012}
}

@techreport{Kim2021Characterization,
author = {Kim, Yongwoon and Yoo, Sangkeun and Lee, Hyunjeong and Han, Soonhung},
doi = {10.22648/ETRI.2020.B.000017},
institution = {ETRI},
keywords = {Digital Twin},
mendeley-groups = {2021 Thesis},
publisher = {ETRI},
title = {{Characterization of Digital Twin}},
url = {https://ksp.etri.re.kr/ksp/plan-report/read.htm?id=787},
year = {2021}
}

@misc{DigitalTwin2021Wikimedia,
author = {{Wikimedia Commons}},
mendeley-groups = {2021 Thesis},
title = {{Digital twin Concept}},
url = {https://en.wikipedia.org/wiki/File:Digital_Twin_Concept_of_Grieves_and_Vickers.png},
urldate = {2021-05-10},
year = {2021}
}

@inbook{Grieves2019Virtually,
author = {Grieves, Michael},
booktitle = {Complex Systems Engineering: Theory and Practice},
doi = {10.2514/5.9781624105654.0175.0200},
isbn = {978-1624105647},
mendeley-groups = {2021 Thesis},
pages = {175--200},
publisher = {American Institute of Aeronautics and Astronautics},
title = {{Virtually Intelligent Product Systems: Digital and Physical Twins}},
url = {https://www.researchgate.net/publication/334599683_Virtually_Intelligent_Product_Systems_Digital_and_Physical_Twins},
year = {2019}
}

@techreport{KOTI2020Global,
author = {한국교통연구원},
file = {:C\:/Users/Lee/Downloads/Trend_on_Global_Logistics Technology_636_1.pdf:pdf;:C\:/Users/Lee/Downloads/Trend_on_Global_Logistics Technology_636_0.pdf:pdf},
institution = {한국교통연구원},
mendeley-groups = {2021 Thesis},
number = {636},
pages = {23},
title = {글로벌 물류기술 동향},
url = {https://www.koti.re.kr/user/globalLgist/BD_selectGlobalLgist.do?q_sn=654},
volume = {14},
year = {2020}
}

@misc{MicrosoftTeams,
author = {Microsoft},
mendeley-groups = {2021 Thesis},
title = {{Microsoft Teams}},
url = {https://www.microsoft.com/en-us/microsoft-teams/group-chat-software},
urldate = {2021-05-20},
year = {2017}
}
@misc{NaverWorks,
author = {{NAVER Cloud Corp.}},
mendeley-groups = {2021 Thesis},
title = {네이버웍스},
url = {https://naver.worksmobile.com/},
urldate = {2021-05-20},
year = {2015}
}

@misc{Wikimedia2021Cave,
author = {{Wikimedia Commons}},
mendeley-groups = {2021 Thesis},
title = {{Cave automatic virtual environment}},
url = {https://commons.wikimedia.org/wiki/File:CAVE_Crayoland.jpg#/media/File:CAVE_Crayoland.jpg},
urldate = {2021-05-21},
year = {2021}
}
@misc{CNET2018Apple,
author = {CNET},
mendeley-groups = {2021 Thesis},
title = {{Apple ARKit 2 demo at WWDC 2018 - YouTube}},
url = {https://www.youtube.com/watch?v=gZhQCVSvq5E},
urldate = {2021-05-21},
year = {2018}
}
@misc{MagicLeap2021MagicLeap,
author = {{Magic Leap, Inc.}},
mendeley-groups = {2021 Thesis},
title = {{Magic Leap}},
url = {https://www.magicleap.com/en-us},
urldate = {2021-05-21},
year = {2021}
}
@misc{Oculus2021Facebook,
author = {{Facebook Technologies, LLC.}},
mendeley-groups = {2021 Thesis},
title = {{Oculus Quest 2}},
url = {https://www.oculus.com/quest-2/},
urldate = {2021-05-21},
year = {2021}
}

@misc{ARKit,
author = {{Apple Inc.}},
mendeley-groups = {2020 Web3D (Unified XR Content Representation),2021 Thesis},
title = {{ARKit}},
url = {https://developer.apple.com/augmented-reality},
urldate = {2020-07-20},
year = {2017}
}

@misc{SubmarineAccident2021Sunwoo,
author = {원선우},
booktitle = {조선일보},
mendeley-groups = {2021 Thesis},
title = {4500억짜리 최신예 잠수함, 동해상에서 기능고장으로 예인},
url = {https://www.chosun.com/politics/diplomacy-defense/2021/01/23/EWSDYHQY5ZFXLKEY3R5EQF5MFE/},
urldate = {2021-05-24},
year = {2021}
}

@misc{AsymmetricPower2012Wind,
author = {{바람의 향기}},
booktitle = {{유용원의 군사세계}},
mendeley-groups = {2021 Thesis},
title = {비대칭 전력 소형 잠수함},
url = {https://bemil.chosun.com/nbrd/bbs/view.html?b_bbs_id=10040&num=68107},
urldate = {2021-05-24},
year = {2012}
}

@misc{VuforiaChalk,
author = {{PTC Inc.}},
mendeley-groups = {2020 Web3D (Unified XR Content Representation),2021 Thesis},
title = {{Vuforia Chalk}},
url = {https://chalk.vuforia.com},
urldate = {2020-07-17},
year = {2017}
}

@misc{Maxwork,
author = {{MAXST Co. LTD.}},
mendeley-groups = {2021 Thesis},
title = {{MAXWORK}},
url = {https://maxwork.maxst.com/},
urldate = {2021-04-13},
year = {2021}
}

@misc{Maxst,
author = {{MAXST Ltd.}},
mendeley-groups = {2020 Web3D (Unified XR Content Representation)},
title = {{MAXST}},
url = {http://maxst.com},
urldate = {2020-07-17},
year = {2017}
}

@inproceedings{Lin2018ScanNet,
abstract = {Lymph node metastasis is one of the most significant diagnostic indicators in breast cancer, which is traditionally observed under the microscope by pathologists. In recent years, computerized histology diagnosis has become one of the most rapidly expanding directions in the field of medical image computing, which aims to alleviate pathologists' workload and simultaneously reduce misdiagnosis rate. However, automatic detection of lymph node metastases from whole slide images remains a challenging problem, due to the large-scale data with enormous resolutions and existence of hard mimics resulting in a large number of false positives. In this paper, we propose a novel framework by leveraging fully convolutional networks for efficient inference to meet the speed requirement for clinical practice, while reconstructing dense predictions under different offsets for ensuring accurate detection on both microand macro-metastases. Incorporating with the strategies of asynchronous sample prefetching and hard negative mining, the network can be effectively trained. Extensive experiments on the benchmark dataset of 2016 Camelyon Grand Challenge corroborated the efficacy of our method. Compared with the state-of-the-art methods, our method achieved superior performance with a faster speed on the tumor localization task and even surpassed human performance on the WSI classification task.},
author = {Lin, Huangjing and Chen, Hao and Dou, Qi and Wang, Liansheng and Qin, Jing and Heng, Pheng-Ann},
booktitle = {2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
doi = {10.1109/WACV.2018.00065},
file = {:C\:/Users/Lee/Downloads/08354169.pdf:pdf},
mendeley-groups = {2021 Thesis},
pages = {539--546},
title = {{ScanNet: A Fast and Dense Scanning Framework for Metastastic Breast Cancer Detection from Whole-Slide Image}},
url = {http://www.scan-net.org/},
year = {2018}
}

@misc{Kinect,
author = {Microsoft},
mendeley-groups = {2021 Thesis},
title = {{Kinect}},
url = {https://developer.microsoft.com/en-us/windows/kinect/},
urldate = {2021-06-04},
year = {2010}
}

@inproceedings{Gao2020User,
abstract = {In this paper, we present a Mixed Reality (MR) remote collaboration system that enables hybrid view sharing from the local worker to the remote expert for real-time remote guidance. The local worker can share either a 2D first-person view, a 360° live view with a fixed viewpoint, or a 3D free view within a point-cloud reconstruction of the local environment. The remote expert can access these views and freely switch between them in Virtual Reality (VR) for better awareness of the local worker's surroundings. We investigate the remote experts' behaviours while using the hybrid view interface during typical pick-and-place remote guiding tasks. We found that the remote experts prefer to learn the local physical layout and search for the targets with a global perspective from the 3D free view. The results also showed that the experts chose to use the 360° live view with independent view control rather than the 2D first-person view with high-resolution to control the task procedures and check the local worker's actions. Our study informs the viewpoint interface design for a MR remote collaboration system in various guiding scenarios.},
address = {New York, NY, USA},
annote = {In our study, we captured the local physical workspace as a static 3D scene which combined a 3D point cloud (objects on tables) and some pre-defned 3D mesh models (tables and walls) (Figure 3). By sharing this view in the VR space for the remote experts, they could quickly understand the spatial relationship of the items from the local side. However, this 3D view did not support real-time updates during the collaborative tasks.

포인트 클라우드 캡처 및 일부 책상이 벽과 같은 것들은 3D 모델링.
이는 현장에 있는 물건들의 공간적 관계를(spatial relationship of items) 이하는데 도움이 됨.
이 3D 뷰는 협업 중에는 실시간 업데이트가 안됨.},
author = {Gao, Lei and Bai, Huidong and Billinghurst, Mark and Lindeman, Robert W},
booktitle = {32nd Australian Conference on Human-Computer Interaction},
doi = {10.1145/3441000.3441038},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gao et al. - 2020 - User Behaviour Analysis of Mixed Reality Remote Collaboration with a Hybrid View Interface(2).pdf:pdf},
isbn = {9781450389754},
keywords = {Mixed Reality,RGB-D camera,point cloud,remote collaboration,tele-presence},
mendeley-groups = {2021 Thesis},
pages = {629--638},
publisher = {Association for Computing Machinery},
series = {OzCHI '20},
title = {{User Behaviour Analysis of Mixed Reality Remote Collaboration with a Hybrid View Interface}},
url = {https://doi.org/10.1145/3441000.3441038},
year = {2020}
}

@misc{Hololens,
author = {Microsoft},
mendeley-groups = {2021 Thesis},
title = {{Microsoft HoloLens}},
url = {https://www.microsoft.com/en-us/hololens},
urldate = {2021-06-04},
year = {2016}
}

@article{Wang20203DGAM,
abstract = {As Virtual Reality(VR), Augmented Reality(AR), Mixed Reality(MR) technology becomes more accessible, it is important to explore VR/AR/MR technologies that can be used for remote collaboration on physical tasks. Previous research has shown that gesture-based interaction is intuitive and expressive for remote collaboration, and using 3D CAD models can provide clear instructions for assembly tasks. In this paper, therefore, we describe a new MR remote collaboration system which combines the use of gesture and CAD models in a complementary manner. The prototype system enables a remote expert in VR to provide instructions based on 3D gesture and CAD models (3DGAM) for a local worker who uses AR to see these instructions. Using this interface, we conducted a formal user study to explore the effect of sharing 3D gesture and CAD models in an assembly training task. We found that the combination of 3D gesture and CAD models can improve remote collaboration on an assembly task with respect to the performance time and user experience. Finally, we provide some conclusions and directions for future research.},
author = {Wang, Peng and Bai, Xiaoliang and Billinghurst, Mark and Zhang, Shusheng and Wei, Sili and Xu, Guangyao and He, Weiping and Zhang, Xiangyu and Zhang, Jie},
doi = {10.1007/s11042-020-09731-7},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2020 - 3DGAM using 3D gesture and CAD models for training on mixed reality remote collaboration.pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {3D CAD models,Augmented reality,Mixed reality,Physical tasks,Remote collaboration,Sharing gesture},
mendeley-groups = {2021 Thesis},
month = {sep},
pages = {1--26},
publisher = {Springer},
title = {{3DGAM: using 3D gesture and CAD models for training on mixed reality remote collaboration}},
url = {https://doi.org/10.1007/s11042-020-09731-7},
year = {2020}
}

@misc{Sumerian,
author = {{Amazon Web Services Inc.}},
mendeley-groups = {2020 Web3D (Unified XR Content Representation),2021 Thesis},
title = {{Amazon Sumerian}},
url = {https://aws.amazon.com/sumerian},
urldate = {2020-07-17},
year = {2018}
}

@misc{WHATWG2021DOM,
author = {WHATWG},
mendeley-groups = {2021 Thesis},
title = {{DOM Standard}},
url = {https://dom.spec.whatwg.org/},
urldate = {2021-06-24},
year = {2021}
}

@misc{MDN2021IntroductionDOM,
author = {{MDN Web Docs}},
mendeley-groups = {2021 Thesis},
title = {{Introduction to the DOM}},
url = {https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction},
urldate = {2021-06-24},
year = {2021}
}

@misc{ARCore,
author = {{Google Inc.}},
mendeley-groups = {2020 Web3D (Unified XR Content Representation),2021 Thesis},
title = {{ARCore}},
url = {https://developers.google.com/ar},
urldate = {2020-07-20},
year = {2018}
}

@article{Teo2020Exploring,
abstract = {Remote collaboration using mixed reality (MR) enables two separated workers to collaborate by sharing visual cues. A local worker can share his/her environment to the remote worker for a better contextual understanding. However, prior techniques were using either 360 video sharing or a complicated 3D reconstruction configuration. This limits the interactivity and practicality of the system. In this paper we show an interactive and easy-to-configure MR remote collaboration technique enabling a local worker to easily share his/her environment by integrating 360 panorama images into a low-cost 3D reconstructed scene as photo-bubbles and projective textures. This enables the remote worker to visit past scenes on either an immersive 360 panoramic scenery, or an interactive 3D environment. We developed a prototype and conducted a user study comparing the two modes of how 360 panorama images could be used in a remote collaboration system. Results suggested that both photo-bubbles and projective textures can provide high social presence, co-presence and low cognitive load for solving tasks while each have its advantage and limitations. For example, photo-bubbles are good for a quick navigation inside the 3D environment without depth perception while projective textures are good for spatial understanding but require physical efforts.},
author = {Teo, Theophilus and Norman, Mitchell and Lee, Gun A and Billinghurst, Mark and Adcock, Matt},
doi = {10.1007/s12193-020-00343-x},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Teo et al. - 2020 - Exploring interaction techniques for 360 panoramas inside a 3D reconstructed scene for mixed reality remote collabor.pdf:pdf},
issn = {1783-8738},
journal = {Journal on Multimodal User Interfaces},
mendeley-groups = {2021 Thesis},
number = {4},
pages = {373--385},
publisher = {Springer},
title = {{Exploring interaction techniques for 360 panoramas inside a 3D reconstructed scene for mixed reality remote collaboration}},
url = {https://doi.org/10.1007/s12193-020-00343-x},
volume = {14},
year = {2020}
}

@misc{ARML,
author = {OGC},
mendeley-groups = {2020 Web3D (Unified XR Content Representation),2020 JCDE (XR Collabo System)},
title = {{Augmented Reality Markup Language}},
url = {https://www.ogc.org/standards/arml},
urldate = {2020-07-17},
year = {2010}
}

@misc{ogc2000gml,
author = {OGC},
mendeley-groups = {2020 JCDE (XR Collabo System),2021 Thesis},
title = {{Geography Markup Language}},
url = {https://www.ogc.org/standards/gml},
urldate = {2020-11-12},
year = {2010}
}

@misc{KARML,
author = {{Georgia Tech}},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Georgia Tech - 2011 - KHARMA.pdf:pdf},
mendeley-groups = {2020 Web3D (Unified XR Content Representation),2021 Thesis},
title = {{KHARMA}},
url = {http://kharma.gatech.edu},
urldate = {2020-07-17},
year = {2011}
}

@misc{KML,
author = {{Open Geospatial Consortium}},
mendeley-groups = {2020 Web3D (Unified XR Content Representation),2021 Thesis},
title = {{KML}},
url = {https://www.ogc.org/standards/kml},
urldate = {2020-07-17},
year = {2008}
}

@inproceedings{Behr2009X3DOM,
abstract = {We present a model that allows to directly integrate X3D nodes into HTML5 DOM content. This model tries to fulfill the promise of the HTML5 specification, which references X3D for declarative 3D scenes but does not define a specific integration mode. The goal of this model is to ease the integration of X3D in modern web appli- cations by directly mapping and synchronizing live DOM elements to a X3D scene model. This is a very similar approach to the cur- rent SVG integration model for 2D graphics. Furthermore, we propose a framework that includes a new X3D Profile for the DOM integration. This profile should make imple- mentation simple, but in addition we show that the current X3D run- time model still scales well. A detailed discussion includes DOM integration issues like events, namespaces and scripting. We finally propose an implementation framework that should work with multi- ple browser frontends (e.g. Firefox and WebKit) and different X3D runtime backends. We hope to connect the technologies and the X3D/ W3C commu- nities with this proposal and outline a model, how an integration without plugins could work. Moreover, we hope to inspire further work, which could lead to a native X3D implementation in browsers similar to the SVG implementations today.},
address = {New York, New York, USA},
author = {Behr, Johannes and Eschler, Peter and Jung, Yvonne and Z{\"{o}}llner, Michael},
booktitle = {Proceedings of the 14th International Conference on 3D Web Technology},
doi = {10.1145/1559764.1559784},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pagou, Hallegraeff - 2013 - Proceedings of the 14th International Conference on Harmful Algae.pdf:pdf},
isbn = {9781605584324},
keywords = {DOM,HTML5,X3D,real-time,web integration},
mendeley-groups = {2020 Web3D (Unified XR Content Representation),2020 JCDE (XR Collabo System),2021 Thesis},
pages = {127--135},
publisher = {Association for Computing Machinery},
series = {Web3D '09},
title = {{X3DOM: A DOM-Based HTML5/X3D Integration Model}},
url = {https://doi.org/10.1145/1559764.1559784 http://portal.acm.org/citation.cfm?doid=1559764.1559784},
year = {2009}
}
@misc{X3DOM,
author = {{Fraunhofer Society}},
mendeley-groups = {2020 Web3D (Unified XR Content Representation),2021 Thesis},
title = {x3dom.org},
url = {https://www.x3dom.org},
urldate = {2020-07-17},
year = {2009}
}

@inproceedings{Sons2010XML3D,
abstract = {Web technologies provide the basis to distribute digital information worldwide and in realtime but they have also established the Web as a ubiquitous application platform. The Web evolved from simple text data to include advanced layout, images, audio, and recently streaming video. Today, as our digital environment becomes increasingly three-dimensional (e.g. 3D cinema, 3D video, consumer 3D displays, and high-performance 3D processing even in mobile devices) it becomes obvious that we must extend the core Web technologies to support interactive 3D content.Instead of adapting existing graphics technologies to the Web, XML3D uses a more radical approach: We take today's Web technology and try to find the minimum set of additions that fully support interactive 3D content as an integral part of mixed 2D/3D Web documents.XML3D enables portable cross-platform authoring, distribution, and rendering of and interaction with 3D data. As a declarative approach XML3D fully leverages existing web technologies including HTML, Cascading Style Sheets (CSS), the Document Object Model (DOM), and AJAX for dynamic content. All 3D content is exposed in the DOM, fully supporting DOM scripting and events, thus allowing Web designers to easily apply their existing skills. The design of XML3D is based on modern programmable graphics hardware, e.g. supports efficient mapping to GPUs without maintaining copies. It also leverages a new approach to specify shaders independently of specific rendering techniques or graphics APIs. We demonstrated the feasibility of our approach by integrating XML3D support into two major open browser frameworks from Mozilla and WebKit as well as providing a portable implementation based on JavaScript and WebGL.},
address = {New York, New York, USA},
annote = {1.interal css만 지원하는 것 같아보임
2. css 프로퍼티로 세팅하는 것이아닌 html 태그로 프로퍼티를 정의하고 이를 css문법으로 referencing함
3. transfomraiton만 지원하는 것 같음},
author = {Sons, Kristian and Klein, Felix and Rubinstein, Dmitri and Byelozyorov, Sergiy and Slusallek, Philipp},
booktitle = {Proceedings of the 15th International Conference on Web 3D Technology},
doi = {10.1145/1836049.1836076},
file = {:C\:/Users/Lee/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zuffo et al. - 2010 - Proceedings, Web3D 2010 15th International Conference on Web 3D Technology Los Angeles, California, July 24-25,.pdf:pdf},
isbn = {9781450302098},
keywords = {CSS,DHTML,DOM,HTML5,XML3D,ray tracing,real-time,web integrated},
mendeley-groups = {2020 Web3D (Unified XR Content Representation),2020 JCDE (XR Collabo System),2021 Thesis},
pages = {175--184},
publisher = {Association for Computing Machinery},
series = {Web3D '10},
title = {{XML3D: Interactive 3D Graphics for the Web}},
url = {https://doi.org/10.1145/1836049.1836076 http://portal.acm.org/citation.cfm?doid=1836049.1836076},
year = {2010}
}

@misc{MDN2021Websocket,
author = {{MDN Web Docs}},
mendeley-groups = {2021 Thesis},
title = {{WebSocket API}},
url = {https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API},
urldate = {2021-06-28},
year = {2021}
}
@misc{Fette2011websocketRFC,
author = {Fette, I. and Melnikov, A.},
booktitle = {Internet Engineering Task Force (IETF)},
mendeley-groups = {2021 Thesis},
title = {rfc6455},
url = {https://datatracker.ietf.org/doc/html/rfc6455},
urldate = {2021-06-28},
year = {2011}
}
